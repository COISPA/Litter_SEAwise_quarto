[
  {
    "objectID": "index.html#introduction",
    "href": "index.html#introduction",
    "title": "Litter modelling & risk analysis",
    "section": "Introduction",
    "text": "Introduction\nThis website presents tools and results for analysing the distribution of marine litter and assessing ecological risks related to fishing activities. The analyses are based on data collected during scientific surveys and use statistical models and biological indicators to describe how litter is distributed across space and time, to estimate the level of hazard for selected species, and to translate this information into exposure risk and impact maps. The overall aim is to provide robust scientific support for marine resource management, combining litter distribution models with multi-species assessments and fleet-specific considerations."
  },
  {
    "objectID": "index.html#github-repository-layout",
    "href": "index.html#github-repository-layout",
    "title": "Litter modelling & risk analysis",
    "section": "GitHub repository layout",
    "text": "GitHub repository layout\n\n00_data_preparation.r – prepares raw data in MEDITS format for the litter distributiojn modelling analiysis and for the risk analysis.\nThe R script:\n\nreads TA (halus) and TL (litter) tables, filters years, converts MEDITS coordinates to decimal degrees with BioIndex::MEDITS.to.dd(); computes mean haul coordinates, swept area, and mean depth; builds unique haul IDs and a continuous time variable ctime (year + day-of-year/365).\nderives post-classification fields on TL (e.g., SUP, FR and ING catecories), ready for later aggregation. The output path is set to input/ under the root directory, further used as source folder of data for the analysis.\n\n01_Litter_analysis_(GAM_analysis).Rmd – litter spatial distribution modelling (R Markdown style).\nThe Rmd file:\n\ndefines user parameters (years ys, AREA, litter category, response index, bootstrap settings, reference month), paths (data_dir, resdir), and grid file.\nloads the observation table, constructs ctime, loads and expands the prediction grid across years/month, and attaches constant swept area to allow effort-scaled predictions.\nfits three candidate GAM models, using Tweedie families for continuous indices and binomial-logit for presence/absence, with REML and gamma = 1.4. Saves model objects, summaries, spline plots, grid predictions and bootstrap time-series (means, 95% CI, CV).\n\n02_Risk_analysis.Rmd – risk analysis and fleet impact.\nThe Rmd file:\n\nloads the litter prediction file produced by the modelling step for the chosen category/response and restricts to selected years/area; merges with the grid and aggregates to per-cell means.\ncomputes species indices (files listed in species_files) and a multi-species index SP (sum of rescaled species, then rescaled to 0–1).\nderives Hazard classes from litter quantiles (default thresholds at 33rd/66th percentiles), plots a hazard map.\nconverts SP to percentiles, combines with Hazard to assign exposure risk (Low/Medium/High), and plots the map.\nmaps fleet-specific Impact by translating Exposure via a gear-impact table (e.g., Low/Medium/High fleets) and plotting per-fleet impact categories."
  },
  {
    "objectID": "index.html#inputs",
    "href": "index.html#inputs",
    "title": "Litter modelling & risk analysis",
    "section": "Inputs",
    "text": "Inputs\n\nTA / TL raw tables (MEDITS format), used in data preparation.\nPrediction grid CSV (e.g., grid_0.05_(0-800m)_GSA_csquare.csv).\nSpecies rasters/tables for abundance (e.g., HKE_GSA18_(abundance)_0.05.csv, MUT_...), specified via species_files / species_names."
  },
  {
    "objectID": "index.html#dependencies",
    "href": "index.html#dependencies",
    "title": "Litter modelling & risk analysis",
    "section": "Dependencies",
    "text": "Dependencies\n\nData prep: BioIndex, lubridate, dplyr.\nModelling: mgcv, mgcViz, MASS, dplyr, ggplot2, sf, rnaturalearth, plus base plotting; Tweedie family used for continuous responses.\nRisk mapping: ggplot2, raster, dplyr, tidyr, maps.\n\nInstall in R (example):\ninstall.packages(c(\n  \"BioIndex\",\"lubridate\",\"dplyr\",\"mgcv\",\"mgcViz\",\"MASS\",\n  \"ggplot2\",\"sf\",\"rnaturalearth\",\"rnaturalearthdata\",\"raster\",\"tidyr\",\"maps\"\n))"
  },
  {
    "objectID": "litter_modelling.html",
    "href": "litter_modelling.html",
    "title": "Modelling analysis on Litter data",
    "section": "",
    "text": "Compiled on 27/08/2025, 12:49"
  },
  {
    "objectID": "litter_modelling.html#user-defined-data",
    "href": "litter_modelling.html#user-defined-data",
    "title": "Modelling analysis on Litter data",
    "section": "User defined data",
    "text": "User defined data\nIn this section every variable that personalises a run is declared, so that the rest of the script can remain untouched and fully generic. First, two paths are set—one pointing to the folder where all outputs will be written (resdir), the other to the directory that holds the litter data post-classified in csv file format. Creating the results folder in advance avoids errors if it does not yet exist. Next come the temporal and spatial filters. ys lists the survey years that should be considered during the analysis, while AREA identifies the geographic unit (for instance a GSA or ICES subarea) to be extracted later from both the observation table and the prediction grid. The response metric is chosen with response. Five alternative indices are available: densities per square kilometre (n_km2, kg_km2), per hour (n_h, kg_h), or simple presence–absence (pa). category variable then tells the script which litter category, among those reclassified, will be isolated from the source data for modelling. Three numeric values follow. nBoot fixes how many Monte-Carlo replicates will be drawn when the script estimates confidence intervals for the annual indices; a larger number increases precision but lengthens computing time. seeds sets the random-number seed so that results are reproducible. mean_sept_area provides the mean swept area of a haul for the particular survey consideredin order to be attached to every cell of the prediction grid so that model outputs can be predicted on the same scale as the observations. Finally, ref_month chooses the calendar month that will define the temporal frame for the prediction grid, allowing the user to centre predictions on the season of interest.\n\n# set results directory\nwd &lt;- \"D:\\\\OneDrive - Coispa Tecnologia & Ricerca S.C.A.R.L\\\\SEAwise\\\\_____ARTICOLO_LITTER\\\\___Analysis_2025___\\\\Litter_SEAwise_quarto\"\nresdir &lt;- file.path(wd, \"output\")\nsuppressWarnings(dir.create(resdir))\n# set directory with data\ndata_dir &lt;- file.path(wd, \"input\")\n\n# set variables\nys &lt;- c(2013:2021,2023,2024) # set the years of data to be used in the analysis\nAREA &lt;- \"18\" # select the geographic area, subarea, GSA for the analysis\nresponse &lt;- \"kg_km2\" # \"n_km2\"  \"kg_km2\" \"n_h\" \"kg_h\" \"pa\"\n\ncategory &lt;- \"FR\"\nnBoot &lt;- 1000 # number of simulation for bootstrapping\nseeds &lt;- 42 # number of seeds for random functions\nmean_sept_area &lt;- 0.05196 # mean swept area value in the survey used to create a grid for model predictions\nref_month &lt;- 7  # reference month for the predictive frid"
  },
  {
    "objectID": "litter_modelling.html#set-grid-file",
    "href": "litter_modelling.html#set-grid-file",
    "title": "Modelling analysis on Litter data",
    "section": "Set grid file",
    "text": "Set grid file\nIn this chunk the user assigns a file name to grid_file. By changing that single line the analyst can substitute a grid of different resolution, spatial extent or depth range without touching any of the statistical code that follows.\n\ngrid_file &lt;- \"grid_0.05_(0-800m)_GSA_csquare.csv\""
  },
  {
    "objectID": "litter_modelling.html#selection-of-response-variable",
    "href": "litter_modelling.html#selection-of-response-variable",
    "title": "Modelling analysis on Litter data",
    "section": "Selection of response variable",
    "text": "Selection of response variable\nThe chunk builds an expression describing the response variable selected by the user. The result, saved in index_expr, ensures that every axis title, legend and plot annotation later in the document automatically displays the correct unit without further editing when the analysis is rerun with a different response variable.\n\n  datasets &lt;- category # name of the dataset to be loaded\n  datasets_label &lt;- cats[cats$cats == category,2]  # Label related to the selected category\n  \n# select response variable\n  \n  index_expr &lt;- switch(response,\n  \"n_km2\" = expression(n / km^2),\n  \"kg_km2\" = expression(kg / km^2),\n  \"n_h\" = \"n/h\",\n  \"kg_h\" = \"kg/h\",\n  \"pa\" = \"p\"\n)"
  },
  {
    "objectID": "litter_modelling.html#data-loading",
    "href": "litter_modelling.html#data-loading",
    "title": "Modelling analysis on Litter data",
    "section": "Data loading",
    "text": "Data loading\nThis step reads the survey file corresponding to the chosen litter category, converts every column name to lower-case for consistency, and inspects the first few rows to confirm that the import succeeded. Immediately afterwards it builds a time variable suitable for smooth modelling. The calendar date of each haul is reconstructed from the year, month and day fields and then converted to “day of year”; dividing by 365 rescales that count onto the interval 0–1, which is easier for a spline to handle. Adding this fractional component back to the integer year yields ctime, a continuous time stamp that increases smoothly from one haul to the next instead of jumping at the turn of each year.\n\n# Load litter data (observations)\ndata &lt;- read.table(file.path(data_dir, paste0(datasets,\"_data.csv\")), sep = \";\", header = TRUE)\ncolnames(data) &lt;- tolower(colnames(data))\nhead(data)\n\n\n  \n\n\n# creation of ctime (continuous time) field\ndate &lt;- paste(data$year,data$month,data$day,sep=\"-\")\ndate &lt;- yday(date)/365\ndata$yday &lt;- date\ndata$ctime &lt;- rowSums(data[,which(colnames(data) %in% c(\"year\",\"yday\"))])\n\nNYEARS &lt;- length(ys)"
  },
  {
    "objectID": "litter_modelling.html#grid-loading",
    "href": "litter_modelling.html#grid-loading",
    "title": "Modelling analysis on Litter data",
    "section": "Grid loading",
    "text": "Grid loading\nThis block imports the spatial prediction grid that underpins all maps and model projections. The grid file is read from the same data directory as the observations, using the file name defined earlier. Immediately after loading, the script filters the grid to retain only the cells whose area code matches the one specified in AREA field, ensuring that any subsequent predictions are confined to the study region of interest. A quick head(grid) prints the first few rows, allowing the user to verify that the grid looks correct before modelling begins.\n\n# Load base grid\ngrid &lt;- read.csv(file.path(data_dir,\"grid_0.05_(0-800m)_GSA_csquare.csv\"), sep = \";\")\ncolnames(grid) &lt;- c(\"id\", \"c_square\", \"x\", \"y\", \"AREA\", \"depth\", \"strata\")\ngrid &lt;- grid[grid$AREA == AREA, ]\n\nhead(grid)\n\n\n  \n\n\n\n\nGrid expansion\nAt this stage the script turns the purely spatial grid into a spatio-temporal one by duplicating every cell for each survey year listed in ys. When the loop ends, the expanded grid replaces the original. A synthetic date string (always the first day of the chosen month) is then built for every grid row and transformed into “day of year”; dividing by 365 rescales that value to the unit interval. Adding this fraction to the integer year generates ctime, a continuous time stamp that parallels the one computed for the observations data, so the model can evaluate temporal smooths on the prediction grid as well. Finally, each grid row is informed with the variable swept, filled with the survey’s mean swept-area value. This constant offset allows predicted quantities, which are expressed per unit swept area, to be placed on the same scale as the observation densities.\n\ng &lt;- NULL\nfor (i in 1:length(ys)) {\n  gtemp &lt;- grid\n  gtemp$year &lt;- ys[i]\n  gtemp$month &lt;- ref_month\n  if (i == 1) {\n    g &lt;- gtemp\n  } else {\n    g &lt;- rbind(g, gtemp)\n  }\n}\ngrid &lt;- g\ndate &lt;- paste(grid$year, grid$month, \"01\", sep = \"-\")\ndate &lt;- yday(date) / 365\ngrid$yday &lt;- date\ngrid$ctime &lt;- rowSums(grid[, which(colnames(grid) %in% c(\"year\", \"yday\"))])\ngrid$swept &lt;- mean_sept_area"
  },
  {
    "objectID": "litter_modelling.html#selection-of-the-response-variable",
    "href": "litter_modelling.html#selection-of-the-response-variable",
    "title": "Modelling analysis on Litter data",
    "section": "selection of the response variable",
    "text": "selection of the response variable\nAfter importing the raw haul table the script computes all five candidate response indices in a single step. Two of them (n_km2 and kg_km2) are obtained by dividing the item counts and masses by the swept area of each haul, giving densities per square kilometre. Two more (n_h and kg_h) standardise the same quantities by haul duration, producing densities per hour. A binary presence–absence flag, pa, is also created by assigning a value of one to hauls where at least one item was caught and zero otherwise. This code guarantees that whichever index the user selected in the parameter block (response) will already exist as a column in the dataset used.\n\ndname &lt;- datasets\nd &lt;- data.frame(data)\nd$n_km2 &lt;- d$n / d$swept\nd$kg_km2 &lt;- d$kg / d$swept\nd$n_h &lt;- d$n / d$duration\nd$kg_h &lt;- d$kg / d$duration\nd$pa &lt;- 0\nd[d$n &gt; 0, \"pa\"] &lt;- 1\nd[d$n == 0, \"pa\"] &lt;- 0\nfm[, \"dataset\"] &lt;- dname"
  },
  {
    "objectID": "litter_modelling.html#model-fitting",
    "href": "litter_modelling.html#model-fitting",
    "title": "Modelling analysis on Litter data",
    "section": "Model fitting",
    "text": "Model fitting\nThis first fitting step takes the formula prepared for Model 1 (the version that uses a continuous temporal spline) and feeds it to mgcv::gam. Before the call, a results table called ts_tab is allocated: one row per survey year and separate columns ready to store the three models’ yearly means, confidence limits and coefficients of variation. The object m is set to 1 so the rest of the script knows which row of the formula matrix to pull from. The conditional block chooses the appropriate likelihood according to the response variable the user selected earlier. Presence–absence (pa) triggers a binomial logit specification; any of the continuous density indices triggers the Tweedie (tw()) family. In every case the model is estimated by REML and the smoothing penalty is inflated with gamma = 1.4, helping to prevent over-fitting when data are sparse.\n\nts_tab &lt;- data.frame(matrix(ncol = 13, nrow = length(ys)))\ncolnames(ts_tab) &lt;- c(\"year\", \"mean_1\", \"lower_1\", \"higher_1\", \"mean_2\", \"lower_2\", \"higher_2\", \"mean_3\", \"lower_3\", \"higher_3\", \"cv1\",\"cv2\",\"cv3\")\nm=1  # to select model 1\n\n# fitting\n  if (response == \"pa\") {\n    mod &lt;- suppressWarnings(gam(formula(fm[m, \"formula\"]), family = binomial(\"logit\"), method = \"REML\", data = d, gamma=1.4))\n  } else if (response == \"n_km2\") {\n    mod &lt;- suppressWarnings(gam(formula(fm[m, \"formula\"]), tw(), method = \"REML\", data = d, gamma=1.4))\n  } else if (response == \"kg_km2\") {\n    mod &lt;- suppressWarnings(gam(formula(fm[m, \"formula\"]), method = \"REML\", tw(), data = d, gamma=1.4))\n  } else if (response == \"n_h\") {\n    mod &lt;- suppressWarnings(gam(formula(fm[m, \"formula\"]), tw(), method = \"REML\", data = d, gamma=1.4))\n  } else if (response == \"kg_km2\") {\n    mod &lt;- suppressWarnings(gam(formula(fm[m, \"formula\"]), method = \"REML\", tw(), data = d, gamma=1.4))\n  }\n\n  # save gam model object\n  saveRDS(mod, file.path(resdir, paste0(dname, \"_\", fm[m, \"response\"], \"_\", fm[m, \"mod\"], \"_MODEL.rds\")))"
  },
  {
    "objectID": "litter_modelling.html#saving-model-diagnostics",
    "href": "litter_modelling.html#saving-model-diagnostics",
    "title": "Modelling analysis on Litter data",
    "section": "Saving model diagnostics",
    "text": "Saving model diagnostics\n\n  # save model summary\n  sum &lt;- summary(mod); sum\n\n\nFamily: Tweedie(p=1.665) \nLink function: log \n\nFormula:\nkg_km2 ~ s(x, y, bs = \"ds\", m = c(1, 0.5), k = 128) + s(ctime, \n    k = 11, bs = \"ds\", m = c(1, 0)) + offset(log(swept))\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.40393    0.09264   25.95   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n            edf Ref.df     F p-value    \ns(x,y)   45.031    127 2.106  &lt;2e-16 ***\ns(ctime)  1.884     10 0.436  0.0476 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.0456   Deviance explained =   40%\n-REML = 926.21  Scale est. = 7.0323    n = 953\n\n  sink(file.path(resdir, paste0(dname, \"_\", fm[m, \"response\"], \"_\", fm[m, \"mod\"], \"_summary.txt\")))\n  print(sum)\n\n\nFamily: Tweedie(p=1.665) \nLink function: log \n\nFormula:\nkg_km2 ~ s(x, y, bs = \"ds\", m = c(1, 0.5), k = 128) + s(ctime, \n    k = 11, bs = \"ds\", m = c(1, 0)) + offset(log(swept))\n\nParametric coefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  2.40393    0.09264   25.95   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nApproximate significance of smooth terms:\n            edf Ref.df     F p-value    \ns(x,y)   45.031    127 2.106  &lt;2e-16 ***\ns(ctime)  1.884     10 0.436  0.0476 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nR-sq.(adj) =  0.0456   Deviance explained =   40%\n-REML = 926.21  Scale est. = 7.0323    n = 953\n\n  print(paste(\"AIC: \", AIC(mod)))\n\n[1] \"AIC:  2524.52863629111\"\n\n  sink()\n\n  # store model statistics in fm table\n  fm[m, \"dev.expl\"] &lt;- round(sum$dev.expl * 100, 2)\n  fm[m, \"AIC\"] &lt;- AIC(mod); AIC(mod)\n\n[1] 2524.529\n\n  # visualisation plot of splines\n    par(mfrow = c(1, 2))\n    plot(mod, all.terms = TRUE, scheme = 2, shade = TRUE)\n\n\n\n\n\n\n\n  # save plot of splines\n  b &lt;- getViz(mod)\n  jpeg(paste(resdir, paste0(dname, \"_\", fm[m, 2], \"_\", fm[m, \"mod\"], \"_splines.jpg\"), sep = \"/\"), width = 3000, height = 1500, res = 300)\n      par(mfrow = c(1, 2))\n      plot(mod, all.terms = TRUE, scheme = 2, shade = TRUE)\n  dev.off()\n\npng \n  2"
  },
  {
    "objectID": "litter_modelling.html#model-predictions",
    "href": "litter_modelling.html#model-predictions",
    "title": "Modelling analysis on Litter data",
    "section": "Model predictions",
    "text": "Model predictions\nThis section captures a complete diagnostic picture of the model immediately after it has been fitted. First, summary(mod) is computed and printed to the console so the user can see the principal statistics interactively. The same summary, together with the numeric value of the model’s Akaike Information Criterion, is redirected to a text file for later reference. The model performances are evaluated on the base of percentage deviance explained and AIC, repectivelly reported into the corresponding row of the fm table in order to make later comparisons quicker. The code produces a pair of diagnostic plots that display the fitted spatial smooth and the temporal effect respectivelly. They are first drawn to the current graphics device for on-screen inspection and then exported at high resolution to a JPEG file.\n\n  # predictions + maps plot\n  grid$pred &lt;- NA\n  grid$pred &lt;- predict(mod, newdata = grid, type = \"response\")\n  head(grid)\n\n\n  \n\n\n  # write.table(grid, file.path(data_dir, paste0(\"Litter_\", dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \".csv\")), sep = \";\", row.names = FALSE)\n  write.table(grid, file.path(resdir, paste0(\"Litter_\", dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \".csv\")), sep = \";\", row.names = FALSE)"
  },
  {
    "objectID": "litter_modelling.html#averaging-distribution-maps",
    "href": "litter_modelling.html#averaging-distribution-maps",
    "title": "Modelling analysis on Litter data",
    "section": "Averaging distribution maps",
    "text": "Averaging distribution maps\nThis chunk summarises the model’s spatial predictions into a single map that illustrates how litter density is currently distributed. It first identifies the last three survey years and extracts from the expanded grid only the rows that belong to those years. Grouping by grid cell, it averages the predicted values across the three slices, yielding one mean estimate per cell (mean). Coastlines are added with rnaturalearth so that land–sea boundaries are clear. The map itself is drawn with ggplot2: each grid cell is a tile coloured by its mean predicted density on a Viridis scale. Customised font sizes ensure that labels remain legible in the exported figure. Finally, the graphic is printed to the R graphics device for immediate inspection and saved twice—once as a high-resolution JPEG for reports and once as a serialised ggplot object (.rds) in case the user wants to re-open or modify the map later without re-running the entire script.\n\n  # map of average model estimations for the latest three years\n  last_ys &lt;- sort(ys)[(length(ys)-2):length(ys)]\n  map &lt;- grid[grid$year %in% last_ys, ]\n  map &lt;- map %&gt;% group_by(c_square, x,y) %&gt;% summarise(mean = mean(pred, na.rm=TRUE))\n\n  xmin &lt;- min(map$x)\n  xmax &lt;- max(map$x)\n  ymin &lt;- min(map$y)\n  ymax &lt;- max(map$y)\n  xl &lt;- c(xmin - (xmax - xmin) * 0.05, xmax + (xmax - xmin) * 0.05)\n  yl &lt;- c(ymin - (ymax - ymin) * 0.05, ymax + (ymax - ymin) * 0.05)\n  x_breaks &lt;- c(round(xmin, 0), round(xmin, 0) + round((xmax - xmin) / 2, 0), round(xmin, 0) + 2 * round((xmax - xmin) / 2, 0))\n  y_breaks &lt;- c(round(ymin, 0), round(ymin, 0) + round((ymax - ymin) / 2, 0), round(ymin, 0) + 2 * round((ymax - ymin) / 2, 0))\n  \n  library(sf)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\n  world &lt;- ne_countries(scale = \"large\", returnclass = \"sf\")\n  # world &lt;- map_data(\"world\")\np1 &lt;- ggplot() +\n  geom_tile(data = map, aes_string(x = \"x\", y = \"y\", fill = \"mean\")) +\n  scale_fill_viridis_c(option = \"D\", direction = -1) +\n  scale_x_continuous(breaks = x_breaks) +\n  scale_y_continuous(breaks = y_breaks) +\n  geom_sf(data = world, fill = \"lightgrey\", color = \"darkgrey\", linewidth = 0.3) +\n  coord_sf(xlim = xl, ylim = yl, expand = FALSE) +\n  theme_bw() +\n  xlab(\"Longitude\") +\n  ylab(\"Latitude\") +\n  labs(fill = index_expr) +\n  ggtitle(paste0(datasets_label)) +\n  theme(\n    plot.title = element_text(size = 16, hjust = 0.5),     \n    axis.title = element_text(size = 16),                 \n    axis.text = element_text(size = 13),                  \n    legend.title = element_text(size = 14),                \n    legend.text = element_text(size = 13)                 \n  )\n  print(p1)\n\n\n\n\n\n\n\n  ggsave(paste(resdir, paste0(datasets,\"_\",response,\"_\",fm[m,\"mod\"],\"_MAP.jpg\"), sep = \"/\"), dpi = 300, width = 9, height = 7)\n  saveRDS(p1,paste(resdir, paste0(datasets,\"_\",response,\"_\",fm[m,\"mod\"],\"_MAP.rds\"), sep = \"/\"))"
  },
  {
    "objectID": "litter_modelling.html#uncertainty-estimation",
    "href": "litter_modelling.html#uncertainty-estimation",
    "title": "Modelling analysis on Litter data",
    "section": "Uncertainty estimation",
    "text": "Uncertainty estimation\nThe analysis at this stage generates haul-level predictions from the fitted GAM, replacing the observed response column with model-based values so that each haul retains its original covariates and sampling design. Afterwards, it moves on to quantify uncertainty. The code produces values for model predictions at haul level, then, year by year, draws a number (nBoot=1000) random coefficient sets from the GAM’s multivariate-normal posterior. Each draw yields a new set of haul densities; averaging these within the year produces a distribution of possible annual means. From that distribution the script stores the point estimate, a 95 % confidence interval, and a coefficient of variation, placing the results in a summary table.\n\n  # model prediction at the survey stations\n  hauls &lt;- d\n  hauls[,which(colnames(hauls)==response)] &lt;- predict(mod, type = \"response\")\n\n  # simulation from the posterior distribution and calculation of the linear predictor (the right-hand side of GAM equation) for each simulation\n\n  if (fm[m, \"dev.expl\"] &gt; 0) {\n  cat(\"- Estimation of time series' 95% confidence interval: \\n\")\n  cat(paste(dname, \", \", fm[m, 2], \", model\", fm[m, \"mod\"], \"\\n\"))\n\n\n  # create data frame of the time series\n  ts &lt;- data.frame(matrix(ncol = 5, nrow = length(ys)))\n  colnames(ts) &lt;- c(\"year\", \"mean\", \"lower\", \"higher\", \"cv\")\n\n  # predictions by year\n  y=1\n  for (y in 1:length(ys)) {\n    n.row &lt;- nrow(hauls[hauls$year==ys[y],])\n    Xp.1 = predict(mod, newdata = hauls[hauls$year==ys[y],], type = \"lpmatrix\")\n    OS.pos = numeric(nrow(hauls[hauls$year==ys[y],]))\n    terms.pos = terms(mod)\n\n    Xp.1 &lt;- predict(mod, newdata = hauls[hauls$year==ys[y],], type = \"lpmatrix\")\n    brp.1 &lt;- mvrnorm(n = nBoot, coef(mod), mod$Vp)\n    rep1 &lt;- exp(Xp.1 %*% t(brp.1))\n    cv &lt;- round(apply(rep1, 1, function(x) sd(x) / mean(x)), 2)\n    cv &lt;- mean(cv)\n\n    if (!is.null(mod$offset)) {\n      off.num.pos &lt;- attr(terms.pos, \"offset\")\n      for (i in off.num.pos) OS.pos &lt;- OS.pos + eval(attr(terms.pos,\"variables\")[[i + 1]], hauls[hauls$year==ys[y],])\n    }\n    p.1 = Xp.1 %*% coef(mod) + OS.pos\n    gPred = exp(p.1)\n    pred_year = sum(exp(p.1))/n.row\n    brp.1 = mvrnorm(n = nBoot, coef(mod), mod$Vp)\n    OS.pos = matrix(0, nrow(hauls[hauls$year==ys[y],]), nBoot)\n    terms.pos = terms(mod)\n    if (!is.null(mod$offset)) {\n      off.num.pos &lt;- attr(terms.pos, \"offset\")\n      for (i in off.num.pos) OS.pos &lt;- OS.pos + eval(attr(terms.pos,\"variables\")[[i + 1]], hauls[hauls$year==ys[y],])\n    }\n    rep1 = data.frame(exp(Xp.1 %*% t(brp.1) + OS.pos))\n    idxSamp = as.numeric(colSums(rep1))/n.row\n    halpha = (1 - 0.95)/2\n    hi &lt;- quantile(idxSamp, 1 - halpha, na.rm = TRUE)\n    lo &lt;- quantile(idxSamp, halpha, na.rm = TRUE)\n    ts[y, 1] &lt;- ys[y]\n    ts[y, 2] &lt;- pred_year \n    ts[y, 3] &lt;- lo\n    ts[y, 4] &lt;- hi\n    ts[y, 5] &lt;- cv\n  }\n\n  # table containing  time series for all the 3 models\n  if (fm[m,\"mod\"]==1) {\n      ts_tab[,c(1:4)] &lt;- ts[,c(1:4)]\n      ts_tab[,11] &lt;- ts[,5]\n  } else if (fm[m,\"mod\"]==2) {\n      ts_tab[,c(5:7)] &lt;- ts[,c(2:4)]\n      ts_tab[,12] &lt;- ts[,5]\n  } else if (fm[m,\"mod\"]==3) {\n      ts_tab[,c(8:10)] &lt;- ts[,c(2:4)]\n      ts_tab[,13] &lt;- ts[,5]\n  }\n\n  head(ts_tab)\n  }\n\n- Estimation of time series' 95% confidence interval: \nFR ,  kg_km2 , model 1"
  },
  {
    "objectID": "litter_modelling.html#timeseries-plot",
    "href": "litter_modelling.html#timeseries-plot",
    "title": "Modelling analysis on Litter data",
    "section": "Timeseries plot",
    "text": "Timeseries plot\nThe code constructs a time-series figure that displays the model’s annual abundance index together with its uncertainty. it takes the bootstrap table ts, plots each yearly mean as a blue point joined by a blue line, and draws a semi-transparent ribbon spanning the 95 % confidence limits. All survey years are shown on the x-axis. After printing the plot to the screen, the script saves it twice: a high-resolution JPEG for reports and an .rds file that preserves the ggplot object for later editing without having to rerun the analysis.\n\n    ## plot of time series\n    if (fm[m, \"dev.expl\"] &gt; 0) {\n  max_val &lt;- max(ts$higher,na.rm=TRUE)\n  p2 &lt;- ggplot()+\n    geom_point(ts,mapping=aes(year,mean),color=\"blue\")+\n    geom_line(ts,mapping=aes(year,mean),color=\"blue\")+\n    geom_ribbon(ts,mapping=aes(ymin=lower,ymax=higher,x=year), alpha = 0.1)+\n    scale_x_continuous(breaks = ys)+\n    theme_bw() +\n    theme(\n          plot.title = element_text(size = 16, hjust = 0.5),     # titolo grafico\n          axis.title = element_text(size = 16),                  # titoli assi\n          axis.text = element_text(size = 13),                   # etichette assi\n          legend.title = element_text(size = 14),                # titolo legenda\n          legend.text = element_text(size = 13)                  # testo legenda\n      )+\n    xlab(\"Year\") +\n    ylab(index_expr)+\n    ggtitle(paste(datasets_label)) + \n    theme(plot.title = element_text(hjust = 0.5))\n  print(p2)\n  ggsave(paste(resdir, paste0(dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \"_TimeSeries_CI.jpg\"), sep = \"/\"), dpi = 300, width = 9, height = 7)\n  saveRDS(p2,paste(resdir, paste0(dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \"_TimeSeries_CI.rds\"), sep = \"/\"))\n  } # if dev.exp &gt; 0"
  },
  {
    "objectID": "risk_analysis.html",
    "href": "risk_analysis.html",
    "title": "Litter risk analysis",
    "section": "",
    "text": "Compiled on 27/08/2025, 12:50"
  },
  {
    "objectID": "risk_analysis.html#litter-data",
    "href": "risk_analysis.html#litter-data",
    "title": "Litter risk analysis",
    "section": "Litter data",
    "text": "Litter data\n\nLoad litter data\nThe script loads the litter dataset corresponding to the selected category and response variable. The data are filtered by the chosen years, and a unique cell identifier is created by combining the c-square code with the year.\n\nlitter_file &lt;- paste0(\"Litter_\",litter_category,\"_\",response,\"_1.csv\") # Litter data file\nlitter &lt;- read.csv(file.path(resdir, litter_file), sep = \";\")\ncells &lt;- unique(litter$id)\nlitter &lt;- litter[litter$year %in% ys, ]\nlitter$id &lt;- paste(litter$c_square, litter$year, sep = \"_\")\n\n\n\nGrid preparation\nThe grid data are imported and, if specified, filtered by the selected AREA. The grid is then expanded to cover all years of interest, and each cell is assigned a unique identifier (c_square + year). Litter predictions are merged with the grid, missing values are set to zero, and the data are aggregated by spatial cell. The resulting dataset contains mean litter values per cell, stored under the selected litter category.\n\n# Load grid data\ngrid &lt;- read.csv(file.path(data_dir, \"grid_0.05_(0-800m)_GSA_csquare.csv\"), sep = \";\")\ncolnames(grid) &lt;- c(\"id\", \"c_square\", \"x\", \"y\", \"AREA\", \"depth\", \"strata\")\n# Filter grid by AREA if AREA is not NA\nif (!is.na(AREA)) {\n  grid &lt;- grid[grid$AREA %in% AREA, ]\n}\n\n# Expand grid for all selected years\nfor (i in 1:length(ys)) {\n  gtemp &lt;- grid\n  gtemp$year &lt;- ys[i]\n  if (i == 1) {\n    g &lt;- gtemp\n  } else {\n    g &lt;- rbind(g, gtemp)\n  }\n}\ngrid &lt;- g\nid &lt;- paste(grid$c_square, grid$year, sep = \"_\")\ngrid$id &lt;- id\n\ngel &lt;- merge(grid, litter[, c(\"id\", \"pred\")], by.x = \"id\", by.y = \"id\", all.x = TRUE)\ngel$pred[is.na(gel$pred)] &lt;- 0 # Fill missing litter data with 0\n\n# Dynamically summarize columns in `fleets` and the `pred` column\ndata &lt;- gel %&gt;% \n  group_by(c_square, x, y, AREA, depth, strata) %&gt;% \n  summarise(litter = mean(pred, na.rm = TRUE), # Summarize pred column\n    .groups = \"drop\" # Optional: drop grouping after summarization\n  )\n\n# Rename the last column to the litter category\ncolnames(data) &lt;- c(\"c_square\", \"x\", \"y\", \"AREA\", \"depth\", \"strata\", litter_category)\n\n\n\nPlot of litter distribution\nThe script visualizes the spatial distribution of fishery-related litter. Litter values are normalized to a 0–1 scale, and distribution maps are produced. The plots display relative abundance or mass, depending on the selected response variable.\n\n# plot effort\nxmin &lt;- min(data$x)\nxmax &lt;- max(data$x)\nymin &lt;- min(data$y)\nymax &lt;- max(data$y)\nxl &lt;- c(xmin - (xmax - xmin) * 0.05, xmax + (xmax - xmin) * 0.05)\nyl &lt;- c(ymin - (ymax - ymin) * 0.05, ymax + (ymax - ymin) * 0.05)\nx_breaks &lt;- c(round(xmin, 0), round(xmin, 0) + round((xmax - xmin) / 2, 0), round(xmin, 0) + 2 * round((xmax - xmin) / 2, 0))\ny_breaks &lt;- c(round(ymin, 0), round(ymin, 0) + round((ymax - ymin) / 2, 0), round(ymin, 0) + 2 * round((ymax - ymin) / 2, 0))\n\nworld &lt;- map_data(\"world\")\n\n# Rescaled in 0-1 range\ndata[, litter_category] &lt;- (data[, litter_category] - min(data[, litter_category], na.rm = TRUE)) / \n                           (max(data[, litter_category], na.rm = TRUE) - min(data[, litter_category], na.rm = TRUE))\n\ntype &lt;- ifelse(response==\"n_km2\",\"abundance\", \"mass\")\n\nggplot() +\n    coord_sf(xlim = xl, ylim = yl, expand = FALSE) +\n    geom_tile(data = data, aes_string(x = \"x\", y = \"y\", fill = litter_category)) +\n    scale_fill_viridis_c(option = \"D\", direction = -1) +\n    scale_x_continuous(breaks = x_breaks) +\n    scale_y_continuous(breaks = y_breaks) +\n    geom_polygon(data = world, aes(x = long, y = lat, group = group), fill = \"lightgrey\", color = \"darkgrey\") +\n    theme_bw() +\n    theme(\n    plot.title = element_text(size = 16, hjust = 0.5),     \n    axis.title = element_text(size = 16),                 \n    axis.text = element_text(size = 13),                  \n    legend.title = element_text(size = 14),                \n    legend.text = element_text(size = 13)                 \n    )+\n    xlab(\"Longitude\") +\n    ylab(\"Latitude\") +\n    labs(fill = paste(\"Relative\",type, sep = \" \")) +\n    ggtitle(paste0())"
  },
  {
    "objectID": "risk_analysis.html#species-data",
    "href": "risk_analysis.html#species-data",
    "title": "Litter risk analysis",
    "section": "Species data",
    "text": "Species data\nThe script imports species-specific abundance/biomass data (n/km2) for selected species (e.g., European hake – HKE, red mullet – MUT), stored in the data_dir folder. For each species, data are filtered by year, averaged across spatial cells, and normalized to a 0–1 range. The number of species included in the analysis can be modified by updating the variables species_files and species_names accordingly. These datasets are then merged with the litter data, and a combined multispecies index (SP) is calculated by summing the normalized abundances and rescaling the result.\n\n# Define species file names and column names dynamically\nspecies_files &lt;- c(\"HKE_GSA18_(abundance)_0.05.csv\", \"MUT_GSA18_(abundance)_0.05.csv\") # Add more files here  #\nspecies_names &lt;- c(\"HKE\", \"MUT\") # Add corresponding species names here  \n\n# Prepare data for each species\nspecies_data &lt;- list()\ni=1\nfor (i in seq_along(species_files)) {\n  sp &lt;- read.table(file.path(data_dir, species_files[i]), sep = \";\", header = TRUE)\n  sp &lt;- sp[sp$year %in% ys, c(\"c_square\", \"year\", \"pred\")]\n  sp &lt;- sp %&gt;% group_by(c_square) %&gt;% summarise(pred = mean(pred, na.rm = TRUE))\n  colnames(sp)[ncol(sp)] &lt;- species_names[i]\n  # Rescaled in 0-1 range\n  sp[,species_names[i]] &lt;- sp[,species_names[i]] / max(sp[,species_names[i]], na.rm=TRUE )\n  species_data[[i]] &lt;- sp\n}\n\n# Merge species data with the main data\nfor (sp_data in species_data) {\n  data &lt;- merge(data, sp_data, by = \"c_square\", all.x = TRUE)\n}\n\n# Calculate combined species abundance (e.g., mean or sum across all species)\nif (length(species_names)==1) {\n  data$SP &lt;- data[, species_names]\n} else {\n  data$SP &lt;- rowSums(data[, species_names], na.rm = TRUE) # Change to rowMeans if needed\n}\n# Final data ready for analysis\ndata$SP &lt;- data$SP / max(data$SP, na.rm=TRUE)\n\n\nPlot species distribution\nThis step visualizes the spatial distribution of each species included in the analysis, as well as the aggregated multispecies index. For each species, relative abundance maps are generated using a gradient scale, while the multispecies cumulative index is plotted with thresholds highlighting different quantile levels. These maps provide a spatial overview of single-species patterns and cumulative patterns too.\n\n# Define custom titles for each column\ntitle_mapping &lt;- list(\n  \"HKE\" = \"European hake (HKE)\",\n  \"MUT\" = \"Red mullet (MUT)\",\n  \"SP\" = \"Multispecies abundance index\" # always keep this category\n)\n\n# Define plot limits and breaks\nxmin &lt;- min(data$x, na.rm = TRUE)\nxmax &lt;- max(data$x, na.rm = TRUE)\nymin &lt;- min(data$y, na.rm = TRUE)\nymax &lt;- max(data$y, na.rm = TRUE)\nxl &lt;- c(xmin - (xmax - xmin) * 0.05, xmax + (xmax - xmin) * 0.05)\nyl &lt;- c(ymin - (ymax - ymin) * 0.05, ymax + (ymax - ymin) * 0.05)\nx_breaks &lt;- c(round(xmin, 0), round(xmin, 0) + round((xmax - xmin) / 2, 0), round(xmin, 0) + 2 * round((xmax - xmin) / 2, 0))\ny_breaks &lt;- c(round(ymin, 0), round(ymin, 0) + round((ymax - ymin) / 2, 0), round(ymin, 0) + 2 * round((ymax - ymin) / 2, 0))\n\n# Load world map data\nworld &lt;- map_data(\"world\")\n\n# List of columns to plot (species and aggregated data)\ncolumns_to_plot &lt;- c(species_names, \"SP\") # `species_names` contains names of individual species\ndata$SP[is.nan(data$SP)] &lt;- NA\ndata$SPnorm &lt;- data$SP / max(data$SP, na.rm=TRUE)\n\n# Define global min and max across species and aggregated data\nglobal_min &lt;- min(sapply(columns_to_plot, function(col) min(data[[col]], na.rm = TRUE)))\nglobal_max &lt;- max(sapply(columns_to_plot, function(col) max(data[[col]], na.rm = TRUE)))\n\nmuted_colors &lt;- c(\"#eaf3fc\", \"#c7dffc\", \"#94c1f0\", \"#5a9bd6\", \"#2c7bb6\")\n\n# Generate maps for each species and the aggregated data\nfor (col_name in columns_to_plot) {\n  # Calculate local min and max for the column\n  local_min &lt;- min(data[[col_name]], na.rm = TRUE)\n  local_max &lt;- max(data[[col_name]], na.rm = TRUE)\n  \n  # Get the title for the current column\n  plot_title &lt;- title_mapping[[col_name]] # Map column to title\n  \n  p &lt;- ggplot() +\n    coord_sf(xlim = xl, ylim = yl, expand = FALSE) +\n    geom_tile(data = data[!is.na(data[[col_name]]), ], # Exclude NA values for the column\n              aes_string(x = \"x\", y = \"y\", fill = col_name)) +\n    scale_fill_gradientn(\n      colors = muted_colors, # Use the muted palette\n      name = \"relative abundance\", # Consistent legend title\n      trans = \"sqrt\", # Transformation for better visual spread\n      limits = c(local_min, local_max), # Local scale for each map\n      na.value = \"white\" # Handle NA values with white\n    ) +\n    scale_x_continuous(breaks = x_breaks) +\n    scale_y_continuous(breaks = y_breaks) +\n    geom_polygon(data = world, aes(x = long, y = lat, group = group), fill = \"lightgrey\", color = \"darkgrey\") +\n    theme_bw() +\n  theme(\n    plot.title = element_text(size = 16, hjust = 0.5),     \n    axis.title = element_text(size = 16),                 \n    axis.text = element_text(size = 13),                  \n    legend.title = element_text(size = 14),                \n    legend.text = element_text(size = 13)                 \n  )+\n    xlab(\"Longitude\") +\n    ylab(\"Latitude\") +\n    ggtitle(plot_title) # Use custom title\n  print(p)\n}\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n  p &lt;- ggplot() +\n  coord_sf(xlim = xl, ylim = yl, expand = FALSE) +\n  geom_tile(data = data[!is.na(data[[\"SPnorm\"]]), ], # Exclude NA values for the column\n            aes_string(x = \"x\", y = \"y\", fill = \"SPnorm\")) +\n  scale_fill_gradientn(\n    colors = c(\"#92d050\", \"#ffff00\", \"#ffc000\"), # Custom three-color scale\n    breaks = c(0, hazard_thrs[1], hazard_thrs[2], 1), # Custom breaks\n    limits = c(0, 1), # Ensure scale goes from 0 to 1\n    labels = c(\"0\", as.character(hazard_thrs[1]), as.character(hazard_thrs[2]), \"1\"), # Custom labels\n    name = \"quantiles\" # Legend title\n  ) +\n  scale_x_continuous(breaks = x_breaks) +\n  scale_y_continuous(breaks = y_breaks) +\n  geom_polygon(data = world, aes(x = long, y = lat, group = group), fill = \"lightgrey\", color = \"darkgrey\") +\n  theme_bw() +\n  theme(\n    plot.title = element_text(size = 16, hjust = 0.5),     \n    axis.title = element_text(size = 16),                 \n    axis.text = element_text(size = 13),                  \n    legend.title = element_text(size = 14),                \n    legend.text = element_text(size = 13)                 \n  )+\n  xlab(\"Longitude\") +\n  ylab(\"Latitude\") +\n  ggtitle(paste0(\"Multispecies cumulative abundance\")) # Dynamically set title based on column name\n  print(p)"
  }
]