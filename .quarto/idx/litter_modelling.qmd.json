{"title":"Modelling analysis on Litter data","markdown":{"yaml":{"title":"Modelling analysis on Litter data","author":"M. T. Spedicato, W. Zupa"},"headingText":"stampa la data/ora di compilazione in stile Rmd originale","containsRefs":false,"markdown":"\n\n```{r, echo=FALSE}\ncat(sprintf(\"Compiled on %s\\n\\n\", format(Sys.time(), '%d/%m/%Y, %H:%M')))\n```\n\n<!-- Logo in the title -->\n\n<style>\n/* Make the title line a flex row with the logo on the left */\n.title-with-logo {\n  display: flex;\n  align-items: center;\n  gap: 14px;              /* spacing between logo and text */\n  margin-top: 0.5rem;\n  margin-bottom: 1rem;\n}\n.title-with-logo img {\n  width: 120px;           /* adjust logo size as needed */\n  height: auto;\n}\n</style>\n\n<script>\n// Wrap the H1.title with a container and prepend the logo to its left\ndocument.addEventListener(\"DOMContentLoaded\", function(){\n  // find the auto-generated title element\n  var h1 = document.querySelector(\"h1.title\");\n  if(!h1) return;\n\n  // create wrapper\n  var wrap = document.createElement(\"div\");\n  wrap.className = \"title-with-logo\";\n\n  // create logo\n  var img = document.createElement(\"img\");\n  img.src = \"pics/SeaWiseFinal-01.png\";   // relative path from Rmd root\n  img.alt = \"SEAwise logo\";\n\n  // insert: wrap replaces h1, then put logo + h1 inside wrap\n  h1.parentNode.insertBefore(wrap, h1);\n  wrap.appendChild(img);\n  wrap.appendChild(h1);\n});\n</script>\n\n<!-- Links -->\n\n<div id=\"header-links\">\n  <a href=\"https://seawiseproject.org/\" target=\"_blank\">\n    <img src=\"pics/SeaWiseFinal-01.png\" width=\"20\"  height=\"22\" style=\"margin-right:6px; vertical-align:middle;\">\n    SEAwise Website\n  </a>\n  <a href=\"https://github.com/COISPA/litter\" target=\"_blank\">\n    <img src=\"pics/github.svg\" width=\"20\" height=\"22\" style=\"margin-right:6px; vertical-align:middle;\">\n    COISPA Github\n  </a>\n</div>\n\n<!-- start of the script -->\n\n\n# Introduction\n\nThis R Markdown template provides a flexible, parameter‑driven workflow for modelling the spatial and temporal distribution of marine litter recorded during scientific surveys.\nBy editing only the small set of parameters that appears at the beginning of the script, such as the study period, the geographical area covered by the analysis, the litter category to be analysed, the response metric and so on, the same code can be rendered for any new case study without further modifications.\nThe template expects two external csv files.\nThe first file contains the observations: one row for every haul, with the haul date, geographical coordinates in decimal degrees, depth, the number and weight of litter items, and either the swept area or the tow duration so that the raw counts can be converted into density indices.\nThe second file is a prediction grid that covers the study region with regularly spaced points; it stores the c-squares cell id, the centroid' coordinates of each grid cell, depth and any stratification label relevant to the survey design.\nThe resolution of the example grid is 0.05° x 0.05° (\\~5 km).\nThese files can be replaced freely by the user.\nBefore the statistical analysis begins the script derives a set of density indices (as items or kilograms per square kilometre and per hour) together with a binary presence–absence flag.\nFor the chosen response variable the code fits three candidate Generalised Additive Models with the mgcv package.\nAll models include a two‑dimensional smooth of longitude and latitude, while the temporal component is represented in three alternative ways: a thin‑plate spline of the continuous time variable, a simple factor that treats the year as a categorical effect, or a linear trend.\nContinuous density responses are modelled with a Tweedie distribution, whereas presence–absence data are handled with a binomial logit.\nPenalised REML estimation is adopted throughout, and an inflation factor of 1.4 is applied to the smoothing penalty to prevent against overfitting.\nModel performance is summarised by the percentage of deviance explained and by the Akaike Information Criterion.\nOnce each model has been fitted the script predicts the response at each point of the grid for every survey year and stores the results as plain text.\nIt also builds annual indices of abundance by resampling from the multivariate normal distribution of the model coefficients, which provide confidence intervals for the trend.\nFigures of the smooth terms, maps of the most recent three‑year average distribution and time‑series plots with confidence envelopes are saved as high‑resolution images, alongside serialised versions of the fitted models and all diagnostic statistics.\nEverything is written to the user‑defined results directory so that raw data, models and graphics remain clearly separated.\nTo use the template the user only needs to supply an csv file of litter observation by category and a compatible prediction grid, modify the parameter block accordingly to those resources and render the document.\nThe code mostly relies exclusively on CRAN packages (lubridate, dplyr, mgcv, mgcViz, ggplot2, MASS, sf and rnaturalearth) so the workflow can be reproduced on any standard R installation.\n\n```{r include=FALSE}\nrm(list = ls(all = TRUE))\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(mgcv)\nlibrary(mgcViz)\nlibrary(ggplot2)\nlibrary(gdata)\nlibrary(MASS)\n\ncats <- data.frame(\n  cats=c(\"ADV\",\"INGFB\",\"INGSC\",\"ENT\",\"FR\",\n         \"SUP\",\"Plastic\",\"Rubber\",\"Metal\",\n         \"Glass\",\"Natural\",\"Other\"),\n  cats_names=c(\"Advection\",\n               \"Ingestion fish and birds\",\n               \"Ingestion sharks and cetaceans\", \n               \"Entanglement\", \n               \"Fishing related\",\n               \"SUP\",\n               \"Plastic\",\n               \"Rubber\",\n               \"Metal\",\n               \"Glass\",\n               \"Natural\",\n               \"Other\"))\n```\n\n# Description of the analysis\n\n## User defined data\n\nIn this section every variable that personalises a run is declared, so that the rest of the script can remain untouched and fully generic.\nFirst, two paths are set—one pointing to the folder where all outputs will be written (*resdir*), the other to the directory that holds the litter data post-classified in csv file format.\nCreating the results folder in advance avoids errors if it does not yet exist.\nNext come the temporal and spatial filters.\n*ys* lists the survey years that should be considered during the analysis, while *AREA* identifies the geographic unit (for instance a GSA or ICES subarea) to be extracted later from both the observation table and the prediction grid.\nThe response metric is chosen with response.\nFive alternative indices are available: densities per square kilometre (*n_km2*, *kg_km2*), per hour (*n_h*, *kg_h*), or simple presence–absence (*pa*).\n*category* variable then tells the script which litter category, among those reclassified, will be isolated from the source data for modelling.\nThree numeric values follow.\n*nBoot* fixes how many Monte-Carlo replicates will be drawn when the script estimates confidence intervals for the annual indices; a larger number increases precision but lengthens computing time.\n*seeds* sets the random-number seed so that results are reproducible.\n*mean_sept_area* provides the mean swept area of a haul for the particular survey consideredin order to be attached to every cell of the prediction grid so that model outputs can be predicted on the same scale as the observations.\nFinally, *ref_month* chooses the calendar month that will define the temporal frame for the prediction grid, allowing the user to centre predictions on the season of interest.\n\n```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}\n# set results directory\nwd <- \"D:\\\\OneDrive - Coispa Tecnologia & Ricerca S.C.A.R.L\\\\SEAwise\\\\_____ARTICOLO_LITTER\\\\___Analysis_2025___\\\\Litter_SEAwise_quarto\"\nresdir <- file.path(wd, \"output\")\nsuppressWarnings(dir.create(resdir))\n# set directory with data\ndata_dir <- file.path(wd, \"input\")\n\n# set variables\nys <- c(2013:2021,2023,2024) # set the years of data to be used in the analysis\nAREA <- \"18\" # select the geographic area, subarea, GSA for the analysis\nresponse <- \"kg_km2\" # \"n_km2\"  \"kg_km2\" \"n_h\" \"kg_h\" \"pa\"\n\ncategory <- \"FR\"\nnBoot <- 1000 # number of simulation for bootstrapping\nseeds <- 42 # number of seeds for random functions\nmean_sept_area <- 0.05196 # mean swept area value in the survey used to create a grid for model predictions\nref_month <- 7  # reference month for the predictive frid\n```\n\n## Set grid file\n\nIn this chunk the user assigns a file name to grid_file.\nBy changing that single line the analyst can substitute a grid of different resolution, spatial extent or depth range without touching any of the statistical code that follows.\n\n```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}\ngrid_file <- \"grid_0.05_(0-800m)_GSA_csquare.csv\"\n```\n\n## Selection of response variable\n\nThe chunk builds an expression describing the response variable selected by the user.\nThe result, saved in *index_expr*, ensures that every axis title, legend and plot annotation later in the document automatically displays the correct unit without further editing when the analysis is rerun with a different response variable.\n\n```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}\n  datasets <- category # name of the dataset to be loaded\n  datasets_label <- cats[cats$cats == category,2]  # Label related to the selected category\n  \n# select response variable\n  \n  index_expr <- switch(response,\n  \"n_km2\" = expression(n / km^2),\n  \"kg_km2\" = expression(kg / km^2),\n  \"n_h\" = \"n/h\",\n  \"kg_h\" = \"kg/h\",\n  \"pa\" = \"p\"\n)\n\n```\n\n## Data loading\n\nThis step reads the survey file corresponding to the chosen litter category, converts every column name to lower-case for consistency, and inspects the first few rows to confirm that the import succeeded.\nImmediately afterwards it builds a time variable suitable for smooth modelling.\nThe calendar date of each haul is reconstructed from the year, month and day fields and then converted to “day of year”; dividing by 365 rescales that count onto the interval 0–1, which is easier for a spline to handle.\nAdding this fractional component back to the integer year yields *ctime*, a continuous time stamp that increases smoothly from one haul to the next instead of jumping at the turn of each year.\n\n```{r}\n# Load litter data (observations)\ndata <- read.table(file.path(data_dir, paste0(datasets,\"_data.csv\")), sep = \";\", header = TRUE)\ncolnames(data) <- tolower(colnames(data))\nhead(data)\n\n# creation of ctime (continuous time) field\ndate <- paste(data$year,data$month,data$day,sep=\"-\")\ndate <- yday(date)/365\ndata$yday <- date\ndata$ctime <- rowSums(data[,which(colnames(data) %in% c(\"year\",\"yday\"))])\n\nNYEARS <- length(ys)\n```\n\n## Grid loading\n\nThis block imports the spatial prediction grid that underpins all maps and model projections.\nThe grid file is read from the same data directory as the observations, using the file name defined earlier.\nImmediately after loading, the script filters the grid to retain only the cells whose area code matches the one specified in *AREA* field, ensuring that any subsequent predictions are confined to the study region of interest.\nA quick *head(grid)* prints the first few rows, allowing the user to verify that the grid looks correct before modelling begins.\n\n```{r}\n# Load base grid\ngrid <- read.csv(file.path(data_dir,\"grid_0.05_(0-800m)_GSA_csquare.csv\"), sep = \";\")\ncolnames(grid) <- c(\"id\", \"c_square\", \"x\", \"y\", \"AREA\", \"depth\", \"strata\")\ngrid <- grid[grid$AREA == AREA, ]\n\nhead(grid)\n```\n\n### Grid expansion\n\nAt this stage the script turns the purely spatial grid into a spatio-temporal one by duplicating every cell for each survey year listed in *ys*.\nWhen the loop ends, the expanded grid replaces the original.\nA synthetic date string (always the first day of the chosen month) is then built for every grid row and transformed into “day of year”; dividing by 365 rescales that value to the unit interval.\nAdding this fraction to the integer year generates *ctime*, a continuous time stamp that parallels the one computed for the observations data, so the model can evaluate temporal smooths on the prediction grid as well.\nFinally, each grid row is informed with the variable *swept*, filled with the survey’s mean swept-area value.\nThis constant offset allows predicted quantities, which are expressed per unit swept area, to be placed on the same scale as the observation densities.\n\n```{r}\ng <- NULL\nfor (i in 1:length(ys)) {\n  gtemp <- grid\n  gtemp$year <- ys[i]\n  gtemp$month <- ref_month\n  if (i == 1) {\n    g <- gtemp\n  } else {\n    g <- rbind(g, gtemp)\n  }\n}\ngrid <- g\ndate <- paste(grid$year, grid$month, \"01\", sep = \"-\")\ndate <- yday(date) / 365\ngrid$yday <- date\ngrid$ctime <- rowSums(grid[, which(colnames(grid) %in% c(\"year\", \"yday\"))])\ngrid$swept <- mean_sept_area\n```\n\n# Model setup\n\nThis chunk prepares everything the script needs in order to fit and compare the three alternative GAMs.\nIt first builds an empty data‐frame called fm with six colum (formula, response, model number, dataset name, deviance explained, and AIC), so that summary statistics can be filled in after each model is fitted.\nThe next two lines pre-populate the table: rows 1–3 are given the current response variable and the codes to label the three model variants.\nThe core of the block constructs the model formulas themselves.\nEach one shares the same two-dimensional Duchon spline (*s(x, y, bs = 'ds', m = c(1, 0.5), k = 128)*) to describe spatial structure, and each includes *offset(log(swept))* so that predicted values are scaled appropriately for effort.\nThe only difference is how time enters the model:\n\n1.  Continuous smoother: adds *s(ctime, k = NYEARS, bs = 'ds', m = c(1, 0))*, letting a smooth function capture gradual trends over the entire study period.\n2.  Categorical year effect: replaces the spline with *factor(year)*, treating each year as an independent level so that no assumption is made about continuity from one year to the next.\n3.  Linear trend: inserts the single term *ctime*, forcing the temporal effect to be strictly linear.\n\nFinally the three formulas are printed to the console so the analyst can verify them before the fitting step begins.\n\n```{r}\n# create a data frame to store models' statistics\nfm <- data.frame(matrix(ncol = 6, nrow = 0))\ncolnames(fm) <- c(\"formula\", \"response\", \"mod\", \"dataset\", \"dev.expl\", \"AIC\")\nfm[c(1:3), 2] <- response\nfm[c(1:3), 3] <- seq(1:3)\n\n# Definition of model formulas\n## 1. Use of continuous time\nfm[1, 1] <- paste0(response, \" ~ s(x,y,bs = 'ds', m = c(1, 0.5),k=128) + s(ctime, k = \", NYEARS, \", bs = 'ds', m = c(1, 0)) + offset(log(swept))\")\n\n## 2. Year effect instead of spline\nfm[2, 1] <- paste0(response, \" ~ s(x,y,bs = 'ds', m = c(1, 0.5),k=128) + factor(year) + offset(log(swept))\")\n\n## 3. Linear effect of time\nfm[3, 1] <- paste0(response, \" ~ s(x,y,bs = 'ds', m = c(1, 0.5),k=128) + ctime + offset(log(swept))\")\n\nfm[1, 1]\nfm[2, 1]\nfm[3, 1]\n```\n\n## selection of the response variable\n\nAfter importing the raw haul table the script computes all five candidate response indices in a single step.\nTwo of them (*n_km2* and *kg_km2*) are obtained by dividing the item counts and masses by the swept area of each haul, giving densities per square kilometre.\nTwo more (*n_h* and *kg_h*) standardise the same quantities by haul duration, producing densities per hour.\nA binary presence–absence flag, *pa*, is also created by assigning a value of one to hauls where at least one item was caught and zero otherwise.\nThis code guarantees that whichever index the user selected in the parameter block (*response*) will already exist as a column in the dataset used.\n\n```{r}\ndname <- datasets\nd <- data.frame(data)\nd$n_km2 <- d$n / d$swept\nd$kg_km2 <- d$kg / d$swept\nd$n_h <- d$n / d$duration\nd$kg_h <- d$kg / d$duration\nd$pa <- 0\nd[d$n > 0, \"pa\"] <- 1\nd[d$n == 0, \"pa\"] <- 0\nfm[, \"dataset\"] <- dname\n```\n\n# Analysis on Model 1\n\n## Model fitting\n\nThis first fitting step takes the formula prepared for Model 1 (the version that uses a continuous temporal spline) and feeds it to *mgcv::gam*.\nBefore the call, a results table called *ts_tab* is allocated: one row per survey year and separate columns ready to store the three models’ yearly means, confidence limits and coefficients of variation.\nThe object *m* is set to 1 so the rest of the script knows which row of the formula matrix to pull from.\nThe conditional block chooses the appropriate likelihood according to the response variable the user selected earlier.\nPresence–absence (pa) triggers a *binomial logit* specification; any of the continuous density indices triggers the *Tweedie (tw())* family.\nIn every case the model is estimated by REML and the smoothing penalty is inflated with gamma = 1.4, helping to prevent over-fitting when data are sparse.\n\n```{r}\nts_tab <- data.frame(matrix(ncol = 13, nrow = length(ys)))\ncolnames(ts_tab) <- c(\"year\", \"mean_1\", \"lower_1\", \"higher_1\", \"mean_2\", \"lower_2\", \"higher_2\", \"mean_3\", \"lower_3\", \"higher_3\", \"cv1\",\"cv2\",\"cv3\")\nm=1  # to select model 1\n\n# fitting\n  if (response == \"pa\") {\n    mod <- suppressWarnings(gam(formula(fm[m, \"formula\"]), family = binomial(\"logit\"), method = \"REML\", data = d, gamma=1.4))\n  } else if (response == \"n_km2\") {\n    mod <- suppressWarnings(gam(formula(fm[m, \"formula\"]), tw(), method = \"REML\", data = d, gamma=1.4))\n  } else if (response == \"kg_km2\") {\n    mod <- suppressWarnings(gam(formula(fm[m, \"formula\"]), method = \"REML\", tw(), data = d, gamma=1.4))\n  } else if (response == \"n_h\") {\n    mod <- suppressWarnings(gam(formula(fm[m, \"formula\"]), tw(), method = \"REML\", data = d, gamma=1.4))\n  } else if (response == \"kg_km2\") {\n    mod <- suppressWarnings(gam(formula(fm[m, \"formula\"]), method = \"REML\", tw(), data = d, gamma=1.4))\n  }\n\n  # save gam model object\n  saveRDS(mod, file.path(resdir, paste0(dname, \"_\", fm[m, \"response\"], \"_\", fm[m, \"mod\"], \"_MODEL.rds\")))\n```\n\n## Saving model diagnostics\n\n```{r}\n  # save model summary\n  sum <- summary(mod); sum\n  sink(file.path(resdir, paste0(dname, \"_\", fm[m, \"response\"], \"_\", fm[m, \"mod\"], \"_summary.txt\")))\n  print(sum)\n  print(paste(\"AIC: \", AIC(mod)))\n  sink()\n\n  # store model statistics in fm table\n  fm[m, \"dev.expl\"] <- round(sum$dev.expl * 100, 2)\n  fm[m, \"AIC\"] <- AIC(mod); AIC(mod)\n\n  # visualisation plot of splines\n    par(mfrow = c(1, 2))\n    plot(mod, all.terms = TRUE, scheme = 2, shade = TRUE)\n    \n  # save plot of splines\n  b <- getViz(mod)\n  jpeg(paste(resdir, paste0(dname, \"_\", fm[m, 2], \"_\", fm[m, \"mod\"], \"_splines.jpg\"), sep = \"/\"), width = 3000, height = 1500, res = 300)\n      par(mfrow = c(1, 2))\n      plot(mod, all.terms = TRUE, scheme = 2, shade = TRUE)\n  dev.off()\n```\n\n## Model predictions\n\nThis section captures a complete diagnostic picture of the model immediately after it has been fitted.\nFirst, *summary(mod)* is computed and printed to the console so the user can see the principal statistics interactively.\nThe same summary, together with the numeric value of the model’s Akaike Information Criterion, is redirected to a text file for later reference.\nThe model performances are evaluated on the base of percentage deviance explained and AIC, repectivelly reported into the corresponding row of the fm table in order to make later comparisons quicker.\nThe code produces a pair of diagnostic plots that display the fitted spatial smooth and the temporal effect respectivelly.\nThey are first drawn to the current graphics device for on-screen inspection and then exported at high resolution to a JPEG file.\n\n```{r}\n  # predictions + maps plot\n  grid$pred <- NA\n  grid$pred <- predict(mod, newdata = grid, type = \"response\")\n  head(grid)\n  # write.table(grid, file.path(data_dir, paste0(\"Litter_\", dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \".csv\")), sep = \";\", row.names = FALSE)\n  write.table(grid, file.path(resdir, paste0(\"Litter_\", dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \".csv\")), sep = \";\", row.names = FALSE)\n```\n\n## Averaging distribution maps\n\nThis chunk summarises the model’s spatial predictions into a single map that illustrates how litter density is currently distributed.\nIt first identifies the last three survey years and extracts from the expanded grid only the rows that belong to those years.\nGrouping by grid cell, it averages the predicted values across the three slices, yielding one mean estimate per cell (mean).\nCoastlines are added with rnaturalearth so that land–sea boundaries are clear.\nThe map itself is drawn with ggplot2: each grid cell is a tile coloured by its mean predicted density on a Viridis scale.\nCustomised font sizes ensure that labels remain legible in the exported figure.\nFinally, the graphic is printed to the R graphics device for immediate inspection and saved twice—once as a high-resolution JPEG for reports and once as a serialised ggplot object (.rds) in case the user wants to re-open or modify the map later without re-running the entire script.\n\n```{r message=FALSE, warning=FALSE}\n  # map of average model estimations for the latest three years\n  last_ys <- sort(ys)[(length(ys)-2):length(ys)]\n  map <- grid[grid$year %in% last_ys, ]\n  map <- map %>% group_by(c_square, x,y) %>% summarise(mean = mean(pred, na.rm=TRUE))\n\n  xmin <- min(map$x)\n  xmax <- max(map$x)\n  ymin <- min(map$y)\n  ymax <- max(map$y)\n  xl <- c(xmin - (xmax - xmin) * 0.05, xmax + (xmax - xmin) * 0.05)\n  yl <- c(ymin - (ymax - ymin) * 0.05, ymax + (ymax - ymin) * 0.05)\n  x_breaks <- c(round(xmin, 0), round(xmin, 0) + round((xmax - xmin) / 2, 0), round(xmin, 0) + 2 * round((xmax - xmin) / 2, 0))\n  y_breaks <- c(round(ymin, 0), round(ymin, 0) + round((ymax - ymin) / 2, 0), round(ymin, 0) + 2 * round((ymax - ymin) / 2, 0))\n  \n  library(sf)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\n  world <- ne_countries(scale = \"large\", returnclass = \"sf\")\n  # world <- map_data(\"world\")\np1 <- ggplot() +\n  geom_tile(data = map, aes_string(x = \"x\", y = \"y\", fill = \"mean\")) +\n  scale_fill_viridis_c(option = \"D\", direction = -1) +\n  scale_x_continuous(breaks = x_breaks) +\n  scale_y_continuous(breaks = y_breaks) +\n  geom_sf(data = world, fill = \"lightgrey\", color = \"darkgrey\", linewidth = 0.3) +\n  coord_sf(xlim = xl, ylim = yl, expand = FALSE) +\n  theme_bw() +\n  xlab(\"Longitude\") +\n  ylab(\"Latitude\") +\n  labs(fill = index_expr) +\n  ggtitle(paste0(datasets_label)) +\n  theme(\n    plot.title = element_text(size = 16, hjust = 0.5),     \n    axis.title = element_text(size = 16),                 \n    axis.text = element_text(size = 13),                  \n    legend.title = element_text(size = 14),                \n    legend.text = element_text(size = 13)                 \n  )\n  print(p1)\n  ggsave(paste(resdir, paste0(datasets,\"_\",response,\"_\",fm[m,\"mod\"],\"_MAP.jpg\"), sep = \"/\"), dpi = 300, width = 9, height = 7)\n  saveRDS(p1,paste(resdir, paste0(datasets,\"_\",response,\"_\",fm[m,\"mod\"],\"_MAP.rds\"), sep = \"/\"))\n```\n\n## Uncertainty estimation\n\nThe analysis at this stage generates haul-level predictions from the fitted GAM, replacing the observed response column with model-based values so that each haul retains its original covariates and sampling design.\nAfterwards, it moves on to quantify uncertainty.\nThe code produces values for model predictions at haul level, then, year by year, draws a number (*nBoot*=1000) random coefficient sets from the GAM’s multivariate-normal posterior.\nEach draw yields a new set of haul densities; averaging these within the year produces a distribution of possible annual means.\nFrom that distribution the script stores the point estimate, a 95 % confidence interval, and a coefficient of variation, placing the results in a summary table.\n\n```{r}\n  # model prediction at the survey stations\n  hauls <- d\n  hauls[,which(colnames(hauls)==response)] <- predict(mod, type = \"response\")\n\n  # simulation from the posterior distribution and calculation of the linear predictor (the right-hand side of GAM equation) for each simulation\n\n  if (fm[m, \"dev.expl\"] > 0) {\n  cat(\"- Estimation of time series' 95% confidence interval: \\n\")\n  cat(paste(dname, \", \", fm[m, 2], \", model\", fm[m, \"mod\"], \"\\n\"))\n\n\n  # create data frame of the time series\n  ts <- data.frame(matrix(ncol = 5, nrow = length(ys)))\n  colnames(ts) <- c(\"year\", \"mean\", \"lower\", \"higher\", \"cv\")\n\n  # predictions by year\n  y=1\n  for (y in 1:length(ys)) {\n    n.row <- nrow(hauls[hauls$year==ys[y],])\n    Xp.1 = predict(mod, newdata = hauls[hauls$year==ys[y],], type = \"lpmatrix\")\n    OS.pos = numeric(nrow(hauls[hauls$year==ys[y],]))\n    terms.pos = terms(mod)\n\n    Xp.1 <- predict(mod, newdata = hauls[hauls$year==ys[y],], type = \"lpmatrix\")\n    brp.1 <- mvrnorm(n = nBoot, coef(mod), mod$Vp)\n    rep1 <- exp(Xp.1 %*% t(brp.1))\n    cv <- round(apply(rep1, 1, function(x) sd(x) / mean(x)), 2)\n    cv <- mean(cv)\n\n    if (!is.null(mod$offset)) {\n      off.num.pos <- attr(terms.pos, \"offset\")\n      for (i in off.num.pos) OS.pos <- OS.pos + eval(attr(terms.pos,\"variables\")[[i + 1]], hauls[hauls$year==ys[y],])\n    }\n    p.1 = Xp.1 %*% coef(mod) + OS.pos\n    gPred = exp(p.1)\n    pred_year = sum(exp(p.1))/n.row\n    brp.1 = mvrnorm(n = nBoot, coef(mod), mod$Vp)\n    OS.pos = matrix(0, nrow(hauls[hauls$year==ys[y],]), nBoot)\n    terms.pos = terms(mod)\n    if (!is.null(mod$offset)) {\n      off.num.pos <- attr(terms.pos, \"offset\")\n      for (i in off.num.pos) OS.pos <- OS.pos + eval(attr(terms.pos,\"variables\")[[i + 1]], hauls[hauls$year==ys[y],])\n    }\n    rep1 = data.frame(exp(Xp.1 %*% t(brp.1) + OS.pos))\n    idxSamp = as.numeric(colSums(rep1))/n.row\n    halpha = (1 - 0.95)/2\n    hi <- quantile(idxSamp, 1 - halpha, na.rm = TRUE)\n    lo <- quantile(idxSamp, halpha, na.rm = TRUE)\n    ts[y, 1] <- ys[y]\n    ts[y, 2] <- pred_year \n    ts[y, 3] <- lo\n    ts[y, 4] <- hi\n    ts[y, 5] <- cv\n  }\n\n  # table containing  time series for all the 3 models\n  if (fm[m,\"mod\"]==1) {\n      ts_tab[,c(1:4)] <- ts[,c(1:4)]\n      ts_tab[,11] <- ts[,5]\n  } else if (fm[m,\"mod\"]==2) {\n      ts_tab[,c(5:7)] <- ts[,c(2:4)]\n      ts_tab[,12] <- ts[,5]\n  } else if (fm[m,\"mod\"]==3) {\n      ts_tab[,c(8:10)] <- ts[,c(2:4)]\n      ts_tab[,13] <- ts[,5]\n  }\n\n  head(ts_tab)\n  }\n```\n\n## Timeseries plot\n\nThe code constructs a time-series figure that displays the model’s annual abundance index together with its uncertainty.\nit takes the bootstrap table ts, plots each yearly mean as a blue point joined by a blue line, and draws a semi-transparent ribbon spanning the 95 % confidence limits.\nAll survey years are shown on the x-axis.\nAfter printing the plot to the screen, the script saves it twice: a high-resolution JPEG for reports and an .rds file that preserves the ggplot object for later editing without having to rerun the analysis.\n\n```{r}\n    ## plot of time series\n    if (fm[m, \"dev.expl\"] > 0) {\n  max_val <- max(ts$higher,na.rm=TRUE)\n  p2 <- ggplot()+\n    geom_point(ts,mapping=aes(year,mean),color=\"blue\")+\n    geom_line(ts,mapping=aes(year,mean),color=\"blue\")+\n    geom_ribbon(ts,mapping=aes(ymin=lower,ymax=higher,x=year), alpha = 0.1)+\n    scale_x_continuous(breaks = ys)+\n    theme_bw() +\n    theme(\n          plot.title = element_text(size = 16, hjust = 0.5),     # titolo grafico\n          axis.title = element_text(size = 16),                  # titoli assi\n          axis.text = element_text(size = 13),                   # etichette assi\n          legend.title = element_text(size = 14),                # titolo legenda\n          legend.text = element_text(size = 13)                  # testo legenda\n      )+\n    xlab(\"Year\") +\n    ylab(index_expr)+\n    ggtitle(paste(datasets_label)) + \n    theme(plot.title = element_text(hjust = 0.5))\n  print(p2)\n  ggsave(paste(resdir, paste0(dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \"_TimeSeries_CI.jpg\"), sep = \"/\"), dpi = 300, width = 9, height = 7)\n  saveRDS(p2,paste(resdir, paste0(dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \"_TimeSeries_CI.rds\"), sep = \"/\"))\n  } # if dev.exp > 0\n```\n\n# Analysis on Model 2\n\nThe workflow shown for Model 1 is executed again—line for line for Model 2 (spatial smooth + categorical year) and Model 3 (spatial smooth + linear trend).\nEach model is fitted with the correct family, its object and summary are saved, diagnostic splines are plotted, grid-level predictions and three-year maps are saved out, bootstrap time-series indices are generated, and the same JPEG/RDS files are exported.\nAt each pass the master tables fm and ts_tab are updated so that, once all three loops finish, the user have a complete and directly comparable set of outputs for every candidate model.\n\n```{r echo=FALSE, message=FALSE, warning=FALSE}\n\nm = 2 # to select model 2\n\nif (response == \"pa\") {\n  mod <- suppressWarnings(gam(\n    formula(fm[m, \"formula\"]),\n    family = binomial(\"logit\"),\n    method = \"REML\",\n    data = d,\n    gamma = 1.4\n  ))\n} else if (response == \"n_km2\") {\n  mod <- suppressWarnings(gam(\n    formula(fm[m, \"formula\"]),\n    tw(),\n    method = \"REML\",\n    data = d,\n    gamma = 1.4\n  ))\n} else if (response == \"kg_km2\") {\n  mod <- suppressWarnings(gam(\n    formula(fm[m, \"formula\"]),\n    method = \"REML\",\n    tw(),\n    data = d,\n    gamma = 1.4\n  ))\n} else if (response == \"n_h\") {\n  mod <- suppressWarnings(gam(\n    formula(fm[m, \"formula\"]),\n    tw(),\n    method = \"REML\",\n    data = d,\n    gamma = 1.4\n  ))\n} else if (response == \"kg_km2\") {\n  mod <- suppressWarnings(gam(\n    formula(fm[m, \"formula\"]),\n    method = \"REML\",\n    tw(),\n    data = d,\n    gamma = 1.4\n  ))\n}\n\n# save gam model object\nsaveRDS(mod, file.path(resdir, paste0(dname, \"_\", fm[m, \"response\"], \"_\", fm[m, \"mod\"], \"_MODEL.rds\")))\n\n# save model summary\nsum <- summary(mod)\nsum\nsink(file.path(resdir, paste0(dname, \"_\", fm[m, \"response\"], \"_\", fm[m, \"mod\"], \"_summary.txt\")))\nprint(sum)\nprint(paste(\"AIC: \", AIC(mod)))\nsink()\n\n# store model statistics in fm table\nfm[m, \"dev.expl\"] <- round(sum$dev.expl * 100, 2)\nfm[m, \"AIC\"] <- AIC(mod)\nAIC(mod)\n\n# visualisation plot of splines\npar(mfrow = c(1, 2))\nplot(mod,\n     all.terms = TRUE,\n     scheme = 2,\n     shade = TRUE)\n\n# save plot of splines\nb <- getViz(mod)\njpeg(\n  paste(resdir, paste0(dname, \"_\", fm[m, 2], \"_\", fm[m, \"mod\"], \"_splines.jpg\"), sep = \"/\"),\n  width = 3000,\n  height = 1500,\n  res = 300\n)\npar(mfrow = c(1, 2))\nplot(mod,\n     all.terms = TRUE,\n     scheme = 2,\n     shade = TRUE)\ndev.off()\n\n# predictions + maps plot\ngrid$pred <- NA\ngrid$pred <- predict(mod, newdata = grid, type = \"response\")\n# write.table(grid, file.path(data_dir, paste0(\"Litter_\", dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \".csv\")), sep = \";\", row.names = FALSE)\nwrite.table(grid,\n            file.path(resdir, paste0(\"Litter_\", dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \".csv\")),\n            sep = \";\",\n            row.names = FALSE)\n\n# map of average model estimations for the latest three years\nlast_ys <- sort(ys)[(length(ys) - 2):length(ys)]\nmap <- grid[grid$year %in% last_ys, ]\nmap <- map %>% group_by(c_square, x, y) %>% summarise(mean = mean(pred, na.rm =\n                                                                    TRUE))\n\nxmin <- min(map$x)\nxmax <- max(map$x)\nymin <- min(map$y)\nymax <- max(map$y)\nxl <- c(xmin - (xmax - xmin) * 0.05, xmax + (xmax - xmin) * 0.05)\nyl <- c(ymin - (ymax - ymin) * 0.05, ymax + (ymax - ymin) * 0.05)\nx_breaks <- c(round(xmin, 0),\n              round(xmin, 0) + round((xmax - xmin) / 2, 0),\n              round(xmin, 0) + 2 * round((xmax - xmin) / 2, 0))\ny_breaks <- c(round(ymin, 0),\n              round(ymin, 0) + round((ymax - ymin) / 2, 0),\n              round(ymin, 0) + 2 * round((ymax - ymin) / 2, 0))\n\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nworld <- ne_countries(scale = \"large\", returnclass = \"sf\")\n# world <- map_data(\"world\")\np1 <- ggplot() +\n  geom_tile(data = map, aes_string(x = \"x\", y = \"y\", fill = \"mean\")) +\n  scale_fill_viridis_c(option = \"D\", direction = -1) +\n  scale_x_continuous(breaks = x_breaks) +\n  scale_y_continuous(breaks = y_breaks) +\n  geom_sf(\n    data = world,\n    fill = \"lightgrey\",\n    color = \"darkgrey\",\n    linewidth = 0.3\n  ) +\n  coord_sf(xlim = xl,\n           ylim = yl,\n           expand = FALSE) +\n  theme_bw() +\n  xlab(\"Longitude\") +\n  ylab(\"Latitude\") +\n  labs(fill = index_expr) +\n  ggtitle(paste0(datasets_label)) +\n  theme(\n    plot.title = element_text(size = 16, hjust = 0.5),\n    axis.title = element_text(size = 16),\n    axis.text = element_text(size = 13),\n    legend.title = element_text(size = 14),\n    legend.text = element_text(size = 13)\n  )\nprint(p1)\nggsave(\n  paste(\n    resdir,\n    paste0(datasets, \"_\", response, \"_\", fm[m, \"mod\"], \"_MAP.jpg\"),\n    sep = \"/\"\n  ),\n  dpi = 300,\n  width = 9,\n  height = 7\n)\nsaveRDS(p1, paste(\n  resdir,\n  paste0(datasets, \"_\", response, \"_\", fm[m, \"mod\"], \"_MAP.rds\"),\n  sep = \"/\"\n))\n\n# model prediction at the survey stations\nhauls <- d\nhauls[, which(colnames(hauls) == response)] <- predict(mod, type = \"response\")\n\n# simulation from the posterior distribution and calculation of the linear predictor (the right-hand side of GAM equation) for each simulation\n\nif (fm[m, \"dev.expl\"] > 0) {\n  # create data frame of the time series\n  ts <- data.frame(matrix(ncol = 5, nrow = length(ys)))\n  colnames(ts) <- c(\"year\", \"mean\", \"lower\", \"higher\", \"cv\")\n  \n  # predictions by year\n  y = 1\n  for (y in 1:length(ys)) {\n    n.row <- nrow(hauls[hauls$year == ys[y], ])\n    Xp.1 = predict(mod, newdata = hauls[hauls$year == ys[y], ], type = \"lpmatrix\")\n    OS.pos = numeric(nrow(hauls[hauls$year == ys[y], ]))\n    terms.pos = terms(mod)\n    \n    Xp.1 <- predict(mod, newdata = hauls[hauls$year == ys[y], ], type = \"lpmatrix\")\n    brp.1 <- mvrnorm(n = nBoot, coef(mod), mod$Vp)\n    rep1 <- exp(Xp.1 %*% t(brp.1))\n    cv <- round(apply(rep1, 1, function(x)\n      sd(x) / mean(x)), 2)\n    cv <- mean(cv)\n\n    if (!is.null(mod$offset)) {\n      off.num.pos <- attr(terms.pos, \"offset\")\n      for (i in off.num.pos)\n        OS.pos <- OS.pos + eval(attr(terms.pos, \"variables\")[[i + 1]], \n                                hauls[hauls$year ==ys[y], ])\n    }\n    p.1 = Xp.1 %*% coef(mod) + OS.pos\n    gPred = exp(p.1)\n    pred_year = sum(exp(p.1)) / n.row\n    brp.1 = mvrnorm(n = nBoot, coef(mod), mod$Vp)\n    OS.pos = matrix(0, nrow(hauls[hauls$year == ys[y], ]), nBoot)\n    terms.pos = terms(mod)\n    if (!is.null(mod$offset)) {\n      off.num.pos <- attr(terms.pos, \"offset\")\n      for (i in off.num.pos)\n        OS.pos <- OS.pos + eval(attr(terms.pos, \"variables\")[[i + 1]], \n                                hauls[hauls$year == ys[y], ])\n    }\n    rep1 = data.frame(exp(Xp.1 %*% t(brp.1) + OS.pos))\n    idxSamp = as.numeric(colSums(rep1)) / n.row\n    halpha = (1 - 0.95) / 2\n    hi <- quantile(idxSamp, 1 - halpha, na.rm = TRUE)\n    lo <- quantile(idxSamp, halpha, na.rm = TRUE)\n    ts[y, 1] <- ys[y]\n    ts[y, 2] <- pred_year\n    ts[y, 3] <- lo\n    ts[y, 4] <- hi\n    ts[y, 5] <- cv\n  }\n  \n  # table containing  time series for all the 3 models\n  if (fm[m, \"mod\"] == 1) {\n    ts_tab[, c(1:4)] <- ts[, c(1:4)]\n    ts_tab[, 11] <- ts[, 5]\n  } else if (fm[m, \"mod\"] == 2) {\n    ts_tab[, c(5:7)] <- ts[, c(2:4)]\n    ts_tab[, 12] <- ts[, 5]\n  } else if (fm[m, \"mod\"] == 3) {\n    ts_tab[, c(8:10)] <- ts[, c(2:4)]\n    ts_tab[, 13] <- ts[, 5]\n  }\n  \n  \n  ## plot of time series\n  max_val <- max(ts$higher, na.rm = TRUE)\n  p2 <- ggplot() +\n    geom_point(ts, mapping = aes(year, mean), color = \"blue\") +\n    geom_line(ts, mapping = aes(year, mean), color = \"blue\") +\n    geom_ribbon(ts,\n                mapping = aes(ymin = lower, ymax = higher, x = year),\n                alpha = 0.1) +\n    scale_x_continuous(breaks = ys) +\n    theme_bw() +\n    theme(\n      plot.title = element_text(size = 16, hjust = 0.5),\n      axis.title = element_text(size = 16),\n      axis.text = element_text(size = 13),\n      legend.title = element_text(size = 14),\n      legend.text = element_text(size = 13) \n    ) +\n    xlab(\"Year\") +\n    ylab(index_expr) +\n    ggtitle(paste(datasets_label)) +\n    theme(plot.title = element_text(hjust = 0.5))\n  print(p2)\n  ggsave(\n    paste(\n      resdir,\n      paste0(dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \"_TimeSeries_CI.jpg\"),\n      sep = \"/\"\n    ),\n    dpi = 300,\n    width = 7,\n    height = 7\n  )\n  saveRDS(p2, paste(\n    resdir,\n    paste0(dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \"_TimeSeries_CI.rds\"),\n    sep = \"/\"\n  ))\n} # if dev.exp > 0\n\n```\n\n# Analysis on Model 3\n\nThe workflow shown for Model 1 and 2 is executed again—line for line for Model 3 (spatial smooth + linear trend). At each step the master tables *fm* and *ts_tab* are updated so that, once all three loops finish, the user have a complete and directly comparable set of outputs for every candidate model.\n\n```{r echo=FALSE, message=FALSE, warning=FALSE}\nm = 3 # to select model 3\n\nif (response == \"pa\") {\n  mod <- suppressWarnings(gam(\n    formula(fm[m, \"formula\"]),\n    family = binomial(\"logit\"),\n    method = \"REML\",\n    data = d,\n    gamma = 1.4\n  ))\n} else if (response == \"n_km2\") {\n  mod <- suppressWarnings(gam(\n    formula(fm[m, \"formula\"]),\n    tw(),\n    method = \"REML\",\n    data = d,\n    gamma = 1.4\n  ))\n} else if (response == \"kg_km2\") {\n  mod <- suppressWarnings(gam(\n    formula(fm[m, \"formula\"]),\n    method = \"REML\",\n    tw(),\n    data = d,\n    gamma = 1.4\n  ))\n} else if (response == \"n_h\") {\n  mod <- suppressWarnings(gam(\n    formula(fm[m, \"formula\"]),\n    tw(),\n    method = \"REML\",\n    data = d,\n    gamma = 1.4\n  ))\n} else if (response == \"kg_km2\") {\n  mod <- suppressWarnings(gam(\n    formula(fm[m, \"formula\"]),\n    method = \"REML\",\n    tw(),\n    data = d,\n    gamma = 1.4\n  ))\n}\n\n# save gam model object\nsaveRDS(mod, file.path(resdir, paste0(dname, \"_\", fm[m, \"response\"], \"_\", fm[m, \"mod\"], \"_MODEL.rds\")))\n\n# save model summary\nsum <- summary(mod)\nsum\nsink(file.path(resdir, paste0(dname, \"_\", fm[m, \"response\"], \"_\", fm[m, \"mod\"], \"_summary.txt\")))\nprint(sum)\nprint(paste(\"AIC: \", AIC(mod)))\nsink()\n\n# store model statistics in fm table\nfm[m, \"dev.expl\"] <- round(sum$dev.expl * 100, 2)\nfm[m, \"AIC\"] <- AIC(mod)\nAIC(mod)\n\n# visualisation plot of splines\npar(mfrow = c(1, 2))\nplot(mod,\n     all.terms = TRUE,\n     scheme = 2,\n     shade = TRUE)\n\n# save plot of splines\nb <- getViz(mod)\njpeg(\n  paste(resdir, paste0(dname, \"_\", fm[m, 2], \"_\", fm[m, \"mod\"], \"_splines.jpg\"), sep = \"/\"),\n  width = 3000,\n  height = 1500,\n  res = 300\n)\npar(mfrow = c(1, 2))\nplot(mod,\n     all.terms = TRUE,\n     scheme = 2,\n     shade = TRUE)\ndev.off()\n\n# predictions + maps plot\ngrid$pred <- NA\ngrid$pred <- predict(mod, newdata = grid, type = \"response\")\n# write.table(grid, file.path(data_dir, paste0(\"Litter_\", dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \".csv\")), sep = \";\", row.names = FALSE)\nwrite.table(grid,\n            file.path(resdir, paste0(\"Litter_\", dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \".csv\")),\n            sep = \";\",\n            row.names = FALSE)\n\n# map of average model estimations for the latest three years\nlast_ys <- sort(ys)[(length(ys) - 2):length(ys)]\nmap <- grid[grid$year %in% last_ys, ]\nmap <- map %>% group_by(c_square, x, y) %>% summarise(mean = mean(pred, na.rm =\n                                                                    TRUE))\n\nxmin <- min(map$x)\nxmax <- max(map$x)\nymin <- min(map$y)\nymax <- max(map$y)\nxl <- c(xmin - (xmax - xmin) * 0.05, xmax + (xmax - xmin) * 0.05)\nyl <- c(ymin - (ymax - ymin) * 0.05, ymax + (ymax - ymin) * 0.05)\nx_breaks <- c(round(xmin, 0),\n              round(xmin, 0) + round((xmax - xmin) / 2, 0),\n              round(xmin, 0) + 2 * round((xmax - xmin) / 2, 0))\ny_breaks <- c(round(ymin, 0),\n              round(ymin, 0) + round((ymax - ymin) / 2, 0),\n              round(ymin, 0) + 2 * round((ymax - ymin) / 2, 0))\n\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nworld <- ne_countries(scale = \"large\", returnclass = \"sf\")\n# world <- map_data(\"world\")\np1 <- ggplot() +\n  geom_tile(data = map, aes_string(x = \"x\", y = \"y\", fill = \"mean\")) +\n  scale_fill_viridis_c(option = \"D\", direction = -1) +\n  scale_x_continuous(breaks = x_breaks) +\n  scale_y_continuous(breaks = y_breaks) +\n  geom_sf(\n    data = world,\n    fill = \"lightgrey\",\n    color = \"darkgrey\",\n    linewidth = 0.3\n  ) +\n  coord_sf(xlim = xl,\n           ylim = yl,\n           expand = FALSE) +\n  theme_bw() +\n  xlab(\"Longitude\") +\n  ylab(\"Latitude\") +\n  labs(fill = index_expr) +\n  ggtitle(paste0(datasets_label)) +\n  theme(\n    plot.title = element_text(size = 16, hjust = 0.5),\n    axis.title = element_text(size = 16),\n    axis.text = element_text(size = 13),\n    legend.title = element_text(size = 14),\n    legend.text = element_text(size = 13)\n  )\nprint(p1)\nggsave(\n  paste(\n    resdir,\n    paste0(datasets, \"_\", response, \"_\", fm[m, \"mod\"], \"_MAP.jpg\"),\n    sep = \"/\"\n  ),\n  dpi = 300,\n  width = 9,\n  height = 7\n)\nsaveRDS(p1, paste(\n  resdir,\n  paste0(datasets, \"_\", response, \"_\", fm[m, \"mod\"], \"_MAP.rds\"),\n  sep = \"/\"\n))\n\n# model prediction at the survey stations\nhauls <- d\nhauls[, which(colnames(hauls) == response)] <- predict(mod, type = \"response\")\n\n# simulation from the posterior distribution and calculation of the linear predictor (the right-hand side of GAM equation) for each simulation\n\nif (fm[m, \"dev.expl\"] > 0) {\n  # create data frame of the time series\n  ts <- data.frame(matrix(ncol = 5, nrow = length(ys)))\n  colnames(ts) <- c(\"year\", \"mean\", \"lower\", \"higher\", \"cv\")\n  \n  # predictions by year\n  y = 1\n  for (y in 1:length(ys)) {\n    n.row <- nrow(hauls[hauls$year == ys[y], ])\n    Xp.1 = predict(mod, newdata = hauls[hauls$year == ys[y], ], type = \"lpmatrix\")\n    OS.pos = numeric(nrow(hauls[hauls$year == ys[y], ]))\n    terms.pos = terms(mod)\n    \n    Xp.1 <- predict(mod, newdata = hauls[hauls$year == ys[y], ], type = \"lpmatrix\")\n    brp.1 <- mvrnorm(n = nBoot, coef(mod), mod$Vp)\n    rep1 <- exp(Xp.1 %*% t(brp.1))\n    cv <- round(apply(rep1, 1, function(x)\n      sd(x) / mean(x)), 2)\n    cv <- mean(cv)\n\n    if (!is.null(mod$offset)) {\n      off.num.pos <- attr(terms.pos, \"offset\")\n      for (i in off.num.pos)\n        OS.pos <- OS.pos + eval(attr(terms.pos, \"variables\")[[i + 1]], \n                                hauls[hauls$year ==ys[y], ])\n    }\n    p.1 = Xp.1 %*% coef(mod) + OS.pos\n    gPred = exp(p.1)\n    pred_year = sum(exp(p.1)) / n.row\n    brp.1 = mvrnorm(n = nBoot, coef(mod), mod$Vp)\n    OS.pos = matrix(0, nrow(hauls[hauls$year == ys[y], ]), nBoot)\n    terms.pos = terms(mod)\n    if (!is.null(mod$offset)) {\n      off.num.pos <- attr(terms.pos, \"offset\")\n      for (i in off.num.pos)\n        OS.pos <- OS.pos + eval(attr(terms.pos, \"variables\")[[i + 1]], \n                                hauls[hauls$year == ys[y], ])\n    }\n    rep1 = data.frame(exp(Xp.1 %*% t(brp.1) + OS.pos))\n    idxSamp = as.numeric(colSums(rep1)) / n.row\n    halpha = (1 - 0.95) / 2\n    hi <- quantile(idxSamp, 1 - halpha, na.rm = TRUE)\n    lo <- quantile(idxSamp, halpha, na.rm = TRUE)\n    ts[y, 1] <- ys[y]\n    ts[y, 2] <- pred_year\n    ts[y, 3] <- lo\n    ts[y, 4] <- hi\n    ts[y, 5] <- cv\n  }\n  \n  # table containing  time series for all the 3 models\n  if (fm[m, \"mod\"] == 1) {\n    ts_tab[, c(1:4)] <- ts[, c(1:4)]\n    ts_tab[, 11] <- ts[, 5]\n  } else if (fm[m, \"mod\"] == 2) {\n    ts_tab[, c(5:7)] <- ts[, c(2:4)]\n    ts_tab[, 12] <- ts[, 5]\n  } else if (fm[m, \"mod\"] == 3) {\n    ts_tab[, c(8:10)] <- ts[, c(2:4)]\n    ts_tab[, 13] <- ts[, 5]\n  }\n  \n  \n  ## plot of time series\n  max_val <- max(ts$higher, na.rm = TRUE)\n  p2 <- ggplot() +\n    geom_point(ts, mapping = aes(year, mean), color = \"blue\") +\n    geom_line(ts, mapping = aes(year, mean), color = \"blue\") +\n    geom_ribbon(ts,\n                mapping = aes(ymin = lower, ymax = higher, x = year),\n                alpha = 0.1) +\n    scale_x_continuous(breaks = ys) +\n    theme_bw() +\n    theme(\n      plot.title = element_text(size = 16, hjust = 0.5),\n      axis.title = element_text(size = 16),\n      axis.text = element_text(size = 13),\n      legend.title = element_text(size = 14),\n      legend.text = element_text(size = 13) \n    ) +\n    xlab(\"Year\") +\n    ylab(index_expr) +\n    ggtitle(paste(datasets_label)) +\n    theme(plot.title = element_text(hjust = 0.5))\n  print(p2)\n  ggsave(\n    paste(\n      resdir,\n      paste0(dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \"_TimeSeries_CI.jpg\"),\n      sep = \"/\"\n    ),\n    dpi = 300,\n    width = 7,\n    height = 7\n  )\n  saveRDS(p2, paste(\n    resdir,\n    paste0(dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \"_TimeSeries_CI.rds\"),\n    sep = \"/\"\n  ))\n} # if dev.exp > 0\n```\n\n\n# Merging plots and results\n\nAt the very end of the script everything comes together.\nThe bootstrap table *ts_tab*, which now holds the annual indices and confidence limits for all three candidate models, is reshaped into a long format so that the three trajectories are reported on the same column.\n*ggplot* is then called to build a comparative time-series figure.\nEach model’s curve is shown as points and lines, coloured black for Model 1, green for Model 2 and blue for Model 3.\nIf Model 1 produced confidence limits, its ribbon is added in semi-transparent grey beneath the three lines to give a visual gauge of uncertainty.\nAfter rendering to the screen the plot is exported in two forms: a high-resolution JPEG for static reporting and an RDS object that preserves the ggplot object for future editing.\nFinally, two comma-separated files are written to the results directory.\nThe first (e.g. *Results_summary_n_km2_SUP.csv*) stores the fm table, which summarises deviance explained and AIC for each model; the second ( *Time_series_n_km2_SUP.csv*) contains the complete bootstrap table *ts_tab*.\nThese text files give a concise record of model performance and yearly indices that can be inspected or imported into other software without opening R.\n\n```{r}\n# plot time series of the three models together\nts_tab$year <- ys\nts_long <- reshape2::melt(ts_tab[,c(1,2,5,8)],id.vars=\"year\")\nts_long$model <- substr(as.character(ts_long$variable),nchar(as.character(ts_long$variable)),nchar(as.character(ts_long$variable)))\nts_long <- ts_long[!is.na(ts_long$value),]\n\nif (fm[1, \"dev.expl\"] > 0) {\n  p3 <- ggplot()+\n    geom_point(ts_long,mapping=aes(year,value,color=model))+\n    geom_line(ts_long,mapping=aes(year,value,color=model))+\n    scale_color_manual(values = c(\"1\" = \"black\", \"2\" = \"green\",\"3\"=\"blue\"))+\n    geom_ribbon(ts_tab,mapping=aes(ymin=lower_1,ymax=higher_1,x=year), alpha = 0.1)+\n    scale_x_continuous(breaks = ys)+\n    theme_bw() +\n    theme(\n      plot.title = element_text(size = 16, hjust = 0.5),\n      axis.title = element_text(size = 16),\n      axis.text = element_text(size = 13),\n      legend.title = element_text(size = 14),\n      legend.text = element_text(size = 13) \n    ) +\n    xlab(\"Year\") +\n    ylab(index_expr)+\n    ggtitle(paste(datasets_label))+ \n    theme(plot.title = element_text(hjust = 0.5))\n} else {\n  p3 <- ggplot()+\n    geom_point(ts_long,mapping=aes(year,value,color=model))+\n    geom_line(ts_long,mapping=aes(year,value,color=model))+\n    scale_color_manual(values = c(\"1\" = \"black\", \"2\" = \"green\",\"3\"=\"blue\"))+\n    scale_x_continuous(breaks = ys)+\n    theme_bw() +\n    theme(\n      plot.title = element_text(size = 16, hjust = 0.5),\n      axis.title = element_text(size = 16),\n      axis.text = element_text(size = 13),\n      legend.title = element_text(size = 14),\n      legend.text = element_text(size = 13) \n    ) +\n    xlab(\"Year\") +\n    ylab(index_expr)+\n    ggtitle(paste(datasets_label))+ \n    theme(plot.title = element_text(hjust = 0.5))\n}\n\nprint(p3)\nggsave(paste(resdir, paste0(dname, \"_\", response, \"_TimeSeries_CI_models.jpg\"), sep = \"/\"), dpi = 300, width = 9, height = 7)\nsaveRDS(p3,paste(resdir, paste0(dname, \"_\", response, \"_TimeSeries_CI_models.rds\"), sep = \"/\"))\n\nwrite.table(fm, file.path(resdir, paste0(\"Results_summary_\",response,\"_\",datasets,\".csv\")), sep = \";\", row.names = FALSE)\n\nts_tab$area = AREA\n\nwrite.table(ts_tab, file.path(resdir, paste0(\"_Time_series_\",response,\"_\",datasets,\".csv\")), sep = \";\", row.names = FALSE)\n```\n\n","srcMarkdownNoYaml":"\n\n```{r, echo=FALSE}\n# stampa la data/ora di compilazione in stile Rmd originale\ncat(sprintf(\"Compiled on %s\\n\\n\", format(Sys.time(), '%d/%m/%Y, %H:%M')))\n```\n\n<!-- Logo in the title -->\n\n<style>\n/* Make the title line a flex row with the logo on the left */\n.title-with-logo {\n  display: flex;\n  align-items: center;\n  gap: 14px;              /* spacing between logo and text */\n  margin-top: 0.5rem;\n  margin-bottom: 1rem;\n}\n.title-with-logo img {\n  width: 120px;           /* adjust logo size as needed */\n  height: auto;\n}\n</style>\n\n<script>\n// Wrap the H1.title with a container and prepend the logo to its left\ndocument.addEventListener(\"DOMContentLoaded\", function(){\n  // find the auto-generated title element\n  var h1 = document.querySelector(\"h1.title\");\n  if(!h1) return;\n\n  // create wrapper\n  var wrap = document.createElement(\"div\");\n  wrap.className = \"title-with-logo\";\n\n  // create logo\n  var img = document.createElement(\"img\");\n  img.src = \"pics/SeaWiseFinal-01.png\";   // relative path from Rmd root\n  img.alt = \"SEAwise logo\";\n\n  // insert: wrap replaces h1, then put logo + h1 inside wrap\n  h1.parentNode.insertBefore(wrap, h1);\n  wrap.appendChild(img);\n  wrap.appendChild(h1);\n});\n</script>\n\n<!-- Links -->\n\n<div id=\"header-links\">\n  <a href=\"https://seawiseproject.org/\" target=\"_blank\">\n    <img src=\"pics/SeaWiseFinal-01.png\" width=\"20\"  height=\"22\" style=\"margin-right:6px; vertical-align:middle;\">\n    SEAwise Website\n  </a>\n  <a href=\"https://github.com/COISPA/litter\" target=\"_blank\">\n    <img src=\"pics/github.svg\" width=\"20\" height=\"22\" style=\"margin-right:6px; vertical-align:middle;\">\n    COISPA Github\n  </a>\n</div>\n\n<!-- start of the script -->\n\n\n# Introduction\n\nThis R Markdown template provides a flexible, parameter‑driven workflow for modelling the spatial and temporal distribution of marine litter recorded during scientific surveys.\nBy editing only the small set of parameters that appears at the beginning of the script, such as the study period, the geographical area covered by the analysis, the litter category to be analysed, the response metric and so on, the same code can be rendered for any new case study without further modifications.\nThe template expects two external csv files.\nThe first file contains the observations: one row for every haul, with the haul date, geographical coordinates in decimal degrees, depth, the number and weight of litter items, and either the swept area or the tow duration so that the raw counts can be converted into density indices.\nThe second file is a prediction grid that covers the study region with regularly spaced points; it stores the c-squares cell id, the centroid' coordinates of each grid cell, depth and any stratification label relevant to the survey design.\nThe resolution of the example grid is 0.05° x 0.05° (\\~5 km).\nThese files can be replaced freely by the user.\nBefore the statistical analysis begins the script derives a set of density indices (as items or kilograms per square kilometre and per hour) together with a binary presence–absence flag.\nFor the chosen response variable the code fits three candidate Generalised Additive Models with the mgcv package.\nAll models include a two‑dimensional smooth of longitude and latitude, while the temporal component is represented in three alternative ways: a thin‑plate spline of the continuous time variable, a simple factor that treats the year as a categorical effect, or a linear trend.\nContinuous density responses are modelled with a Tweedie distribution, whereas presence–absence data are handled with a binomial logit.\nPenalised REML estimation is adopted throughout, and an inflation factor of 1.4 is applied to the smoothing penalty to prevent against overfitting.\nModel performance is summarised by the percentage of deviance explained and by the Akaike Information Criterion.\nOnce each model has been fitted the script predicts the response at each point of the grid for every survey year and stores the results as plain text.\nIt also builds annual indices of abundance by resampling from the multivariate normal distribution of the model coefficients, which provide confidence intervals for the trend.\nFigures of the smooth terms, maps of the most recent three‑year average distribution and time‑series plots with confidence envelopes are saved as high‑resolution images, alongside serialised versions of the fitted models and all diagnostic statistics.\nEverything is written to the user‑defined results directory so that raw data, models and graphics remain clearly separated.\nTo use the template the user only needs to supply an csv file of litter observation by category and a compatible prediction grid, modify the parameter block accordingly to those resources and render the document.\nThe code mostly relies exclusively on CRAN packages (lubridate, dplyr, mgcv, mgcViz, ggplot2, MASS, sf and rnaturalearth) so the workflow can be reproduced on any standard R installation.\n\n```{r include=FALSE}\nrm(list = ls(all = TRUE))\nlibrary(lubridate)\nlibrary(dplyr)\nlibrary(mgcv)\nlibrary(mgcViz)\nlibrary(ggplot2)\nlibrary(gdata)\nlibrary(MASS)\n\ncats <- data.frame(\n  cats=c(\"ADV\",\"INGFB\",\"INGSC\",\"ENT\",\"FR\",\n         \"SUP\",\"Plastic\",\"Rubber\",\"Metal\",\n         \"Glass\",\"Natural\",\"Other\"),\n  cats_names=c(\"Advection\",\n               \"Ingestion fish and birds\",\n               \"Ingestion sharks and cetaceans\", \n               \"Entanglement\", \n               \"Fishing related\",\n               \"SUP\",\n               \"Plastic\",\n               \"Rubber\",\n               \"Metal\",\n               \"Glass\",\n               \"Natural\",\n               \"Other\"))\n```\n\n# Description of the analysis\n\n## User defined data\n\nIn this section every variable that personalises a run is declared, so that the rest of the script can remain untouched and fully generic.\nFirst, two paths are set—one pointing to the folder where all outputs will be written (*resdir*), the other to the directory that holds the litter data post-classified in csv file format.\nCreating the results folder in advance avoids errors if it does not yet exist.\nNext come the temporal and spatial filters.\n*ys* lists the survey years that should be considered during the analysis, while *AREA* identifies the geographic unit (for instance a GSA or ICES subarea) to be extracted later from both the observation table and the prediction grid.\nThe response metric is chosen with response.\nFive alternative indices are available: densities per square kilometre (*n_km2*, *kg_km2*), per hour (*n_h*, *kg_h*), or simple presence–absence (*pa*).\n*category* variable then tells the script which litter category, among those reclassified, will be isolated from the source data for modelling.\nThree numeric values follow.\n*nBoot* fixes how many Monte-Carlo replicates will be drawn when the script estimates confidence intervals for the annual indices; a larger number increases precision but lengthens computing time.\n*seeds* sets the random-number seed so that results are reproducible.\n*mean_sept_area* provides the mean swept area of a haul for the particular survey consideredin order to be attached to every cell of the prediction grid so that model outputs can be predicted on the same scale as the observations.\nFinally, *ref_month* chooses the calendar month that will define the temporal frame for the prediction grid, allowing the user to centre predictions on the season of interest.\n\n```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}\n# set results directory\nwd <- \"D:\\\\OneDrive - Coispa Tecnologia & Ricerca S.C.A.R.L\\\\SEAwise\\\\_____ARTICOLO_LITTER\\\\___Analysis_2025___\\\\Litter_SEAwise_quarto\"\nresdir <- file.path(wd, \"output\")\nsuppressWarnings(dir.create(resdir))\n# set directory with data\ndata_dir <- file.path(wd, \"input\")\n\n# set variables\nys <- c(2013:2021,2023,2024) # set the years of data to be used in the analysis\nAREA <- \"18\" # select the geographic area, subarea, GSA for the analysis\nresponse <- \"kg_km2\" # \"n_km2\"  \"kg_km2\" \"n_h\" \"kg_h\" \"pa\"\n\ncategory <- \"FR\"\nnBoot <- 1000 # number of simulation for bootstrapping\nseeds <- 42 # number of seeds for random functions\nmean_sept_area <- 0.05196 # mean swept area value in the survey used to create a grid for model predictions\nref_month <- 7  # reference month for the predictive frid\n```\n\n## Set grid file\n\nIn this chunk the user assigns a file name to grid_file.\nBy changing that single line the analyst can substitute a grid of different resolution, spatial extent or depth range without touching any of the statistical code that follows.\n\n```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}\ngrid_file <- \"grid_0.05_(0-800m)_GSA_csquare.csv\"\n```\n\n## Selection of response variable\n\nThe chunk builds an expression describing the response variable selected by the user.\nThe result, saved in *index_expr*, ensures that every axis title, legend and plot annotation later in the document automatically displays the correct unit without further editing when the analysis is rerun with a different response variable.\n\n```{r echo=TRUE, message=FALSE, warning=FALSE, paged.print=TRUE}\n  datasets <- category # name of the dataset to be loaded\n  datasets_label <- cats[cats$cats == category,2]  # Label related to the selected category\n  \n# select response variable\n  \n  index_expr <- switch(response,\n  \"n_km2\" = expression(n / km^2),\n  \"kg_km2\" = expression(kg / km^2),\n  \"n_h\" = \"n/h\",\n  \"kg_h\" = \"kg/h\",\n  \"pa\" = \"p\"\n)\n\n```\n\n## Data loading\n\nThis step reads the survey file corresponding to the chosen litter category, converts every column name to lower-case for consistency, and inspects the first few rows to confirm that the import succeeded.\nImmediately afterwards it builds a time variable suitable for smooth modelling.\nThe calendar date of each haul is reconstructed from the year, month and day fields and then converted to “day of year”; dividing by 365 rescales that count onto the interval 0–1, which is easier for a spline to handle.\nAdding this fractional component back to the integer year yields *ctime*, a continuous time stamp that increases smoothly from one haul to the next instead of jumping at the turn of each year.\n\n```{r}\n# Load litter data (observations)\ndata <- read.table(file.path(data_dir, paste0(datasets,\"_data.csv\")), sep = \";\", header = TRUE)\ncolnames(data) <- tolower(colnames(data))\nhead(data)\n\n# creation of ctime (continuous time) field\ndate <- paste(data$year,data$month,data$day,sep=\"-\")\ndate <- yday(date)/365\ndata$yday <- date\ndata$ctime <- rowSums(data[,which(colnames(data) %in% c(\"year\",\"yday\"))])\n\nNYEARS <- length(ys)\n```\n\n## Grid loading\n\nThis block imports the spatial prediction grid that underpins all maps and model projections.\nThe grid file is read from the same data directory as the observations, using the file name defined earlier.\nImmediately after loading, the script filters the grid to retain only the cells whose area code matches the one specified in *AREA* field, ensuring that any subsequent predictions are confined to the study region of interest.\nA quick *head(grid)* prints the first few rows, allowing the user to verify that the grid looks correct before modelling begins.\n\n```{r}\n# Load base grid\ngrid <- read.csv(file.path(data_dir,\"grid_0.05_(0-800m)_GSA_csquare.csv\"), sep = \";\")\ncolnames(grid) <- c(\"id\", \"c_square\", \"x\", \"y\", \"AREA\", \"depth\", \"strata\")\ngrid <- grid[grid$AREA == AREA, ]\n\nhead(grid)\n```\n\n### Grid expansion\n\nAt this stage the script turns the purely spatial grid into a spatio-temporal one by duplicating every cell for each survey year listed in *ys*.\nWhen the loop ends, the expanded grid replaces the original.\nA synthetic date string (always the first day of the chosen month) is then built for every grid row and transformed into “day of year”; dividing by 365 rescales that value to the unit interval.\nAdding this fraction to the integer year generates *ctime*, a continuous time stamp that parallels the one computed for the observations data, so the model can evaluate temporal smooths on the prediction grid as well.\nFinally, each grid row is informed with the variable *swept*, filled with the survey’s mean swept-area value.\nThis constant offset allows predicted quantities, which are expressed per unit swept area, to be placed on the same scale as the observation densities.\n\n```{r}\ng <- NULL\nfor (i in 1:length(ys)) {\n  gtemp <- grid\n  gtemp$year <- ys[i]\n  gtemp$month <- ref_month\n  if (i == 1) {\n    g <- gtemp\n  } else {\n    g <- rbind(g, gtemp)\n  }\n}\ngrid <- g\ndate <- paste(grid$year, grid$month, \"01\", sep = \"-\")\ndate <- yday(date) / 365\ngrid$yday <- date\ngrid$ctime <- rowSums(grid[, which(colnames(grid) %in% c(\"year\", \"yday\"))])\ngrid$swept <- mean_sept_area\n```\n\n# Model setup\n\nThis chunk prepares everything the script needs in order to fit and compare the three alternative GAMs.\nIt first builds an empty data‐frame called fm with six colum (formula, response, model number, dataset name, deviance explained, and AIC), so that summary statistics can be filled in after each model is fitted.\nThe next two lines pre-populate the table: rows 1–3 are given the current response variable and the codes to label the three model variants.\nThe core of the block constructs the model formulas themselves.\nEach one shares the same two-dimensional Duchon spline (*s(x, y, bs = 'ds', m = c(1, 0.5), k = 128)*) to describe spatial structure, and each includes *offset(log(swept))* so that predicted values are scaled appropriately for effort.\nThe only difference is how time enters the model:\n\n1.  Continuous smoother: adds *s(ctime, k = NYEARS, bs = 'ds', m = c(1, 0))*, letting a smooth function capture gradual trends over the entire study period.\n2.  Categorical year effect: replaces the spline with *factor(year)*, treating each year as an independent level so that no assumption is made about continuity from one year to the next.\n3.  Linear trend: inserts the single term *ctime*, forcing the temporal effect to be strictly linear.\n\nFinally the three formulas are printed to the console so the analyst can verify them before the fitting step begins.\n\n```{r}\n# create a data frame to store models' statistics\nfm <- data.frame(matrix(ncol = 6, nrow = 0))\ncolnames(fm) <- c(\"formula\", \"response\", \"mod\", \"dataset\", \"dev.expl\", \"AIC\")\nfm[c(1:3), 2] <- response\nfm[c(1:3), 3] <- seq(1:3)\n\n# Definition of model formulas\n## 1. Use of continuous time\nfm[1, 1] <- paste0(response, \" ~ s(x,y,bs = 'ds', m = c(1, 0.5),k=128) + s(ctime, k = \", NYEARS, \", bs = 'ds', m = c(1, 0)) + offset(log(swept))\")\n\n## 2. Year effect instead of spline\nfm[2, 1] <- paste0(response, \" ~ s(x,y,bs = 'ds', m = c(1, 0.5),k=128) + factor(year) + offset(log(swept))\")\n\n## 3. Linear effect of time\nfm[3, 1] <- paste0(response, \" ~ s(x,y,bs = 'ds', m = c(1, 0.5),k=128) + ctime + offset(log(swept))\")\n\nfm[1, 1]\nfm[2, 1]\nfm[3, 1]\n```\n\n## selection of the response variable\n\nAfter importing the raw haul table the script computes all five candidate response indices in a single step.\nTwo of them (*n_km2* and *kg_km2*) are obtained by dividing the item counts and masses by the swept area of each haul, giving densities per square kilometre.\nTwo more (*n_h* and *kg_h*) standardise the same quantities by haul duration, producing densities per hour.\nA binary presence–absence flag, *pa*, is also created by assigning a value of one to hauls where at least one item was caught and zero otherwise.\nThis code guarantees that whichever index the user selected in the parameter block (*response*) will already exist as a column in the dataset used.\n\n```{r}\ndname <- datasets\nd <- data.frame(data)\nd$n_km2 <- d$n / d$swept\nd$kg_km2 <- d$kg / d$swept\nd$n_h <- d$n / d$duration\nd$kg_h <- d$kg / d$duration\nd$pa <- 0\nd[d$n > 0, \"pa\"] <- 1\nd[d$n == 0, \"pa\"] <- 0\nfm[, \"dataset\"] <- dname\n```\n\n# Analysis on Model 1\n\n## Model fitting\n\nThis first fitting step takes the formula prepared for Model 1 (the version that uses a continuous temporal spline) and feeds it to *mgcv::gam*.\nBefore the call, a results table called *ts_tab* is allocated: one row per survey year and separate columns ready to store the three models’ yearly means, confidence limits and coefficients of variation.\nThe object *m* is set to 1 so the rest of the script knows which row of the formula matrix to pull from.\nThe conditional block chooses the appropriate likelihood according to the response variable the user selected earlier.\nPresence–absence (pa) triggers a *binomial logit* specification; any of the continuous density indices triggers the *Tweedie (tw())* family.\nIn every case the model is estimated by REML and the smoothing penalty is inflated with gamma = 1.4, helping to prevent over-fitting when data are sparse.\n\n```{r}\nts_tab <- data.frame(matrix(ncol = 13, nrow = length(ys)))\ncolnames(ts_tab) <- c(\"year\", \"mean_1\", \"lower_1\", \"higher_1\", \"mean_2\", \"lower_2\", \"higher_2\", \"mean_3\", \"lower_3\", \"higher_3\", \"cv1\",\"cv2\",\"cv3\")\nm=1  # to select model 1\n\n# fitting\n  if (response == \"pa\") {\n    mod <- suppressWarnings(gam(formula(fm[m, \"formula\"]), family = binomial(\"logit\"), method = \"REML\", data = d, gamma=1.4))\n  } else if (response == \"n_km2\") {\n    mod <- suppressWarnings(gam(formula(fm[m, \"formula\"]), tw(), method = \"REML\", data = d, gamma=1.4))\n  } else if (response == \"kg_km2\") {\n    mod <- suppressWarnings(gam(formula(fm[m, \"formula\"]), method = \"REML\", tw(), data = d, gamma=1.4))\n  } else if (response == \"n_h\") {\n    mod <- suppressWarnings(gam(formula(fm[m, \"formula\"]), tw(), method = \"REML\", data = d, gamma=1.4))\n  } else if (response == \"kg_km2\") {\n    mod <- suppressWarnings(gam(formula(fm[m, \"formula\"]), method = \"REML\", tw(), data = d, gamma=1.4))\n  }\n\n  # save gam model object\n  saveRDS(mod, file.path(resdir, paste0(dname, \"_\", fm[m, \"response\"], \"_\", fm[m, \"mod\"], \"_MODEL.rds\")))\n```\n\n## Saving model diagnostics\n\n```{r}\n  # save model summary\n  sum <- summary(mod); sum\n  sink(file.path(resdir, paste0(dname, \"_\", fm[m, \"response\"], \"_\", fm[m, \"mod\"], \"_summary.txt\")))\n  print(sum)\n  print(paste(\"AIC: \", AIC(mod)))\n  sink()\n\n  # store model statistics in fm table\n  fm[m, \"dev.expl\"] <- round(sum$dev.expl * 100, 2)\n  fm[m, \"AIC\"] <- AIC(mod); AIC(mod)\n\n  # visualisation plot of splines\n    par(mfrow = c(1, 2))\n    plot(mod, all.terms = TRUE, scheme = 2, shade = TRUE)\n    \n  # save plot of splines\n  b <- getViz(mod)\n  jpeg(paste(resdir, paste0(dname, \"_\", fm[m, 2], \"_\", fm[m, \"mod\"], \"_splines.jpg\"), sep = \"/\"), width = 3000, height = 1500, res = 300)\n      par(mfrow = c(1, 2))\n      plot(mod, all.terms = TRUE, scheme = 2, shade = TRUE)\n  dev.off()\n```\n\n## Model predictions\n\nThis section captures a complete diagnostic picture of the model immediately after it has been fitted.\nFirst, *summary(mod)* is computed and printed to the console so the user can see the principal statistics interactively.\nThe same summary, together with the numeric value of the model’s Akaike Information Criterion, is redirected to a text file for later reference.\nThe model performances are evaluated on the base of percentage deviance explained and AIC, repectivelly reported into the corresponding row of the fm table in order to make later comparisons quicker.\nThe code produces a pair of diagnostic plots that display the fitted spatial smooth and the temporal effect respectivelly.\nThey are first drawn to the current graphics device for on-screen inspection and then exported at high resolution to a JPEG file.\n\n```{r}\n  # predictions + maps plot\n  grid$pred <- NA\n  grid$pred <- predict(mod, newdata = grid, type = \"response\")\n  head(grid)\n  # write.table(grid, file.path(data_dir, paste0(\"Litter_\", dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \".csv\")), sep = \";\", row.names = FALSE)\n  write.table(grid, file.path(resdir, paste0(\"Litter_\", dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \".csv\")), sep = \";\", row.names = FALSE)\n```\n\n## Averaging distribution maps\n\nThis chunk summarises the model’s spatial predictions into a single map that illustrates how litter density is currently distributed.\nIt first identifies the last three survey years and extracts from the expanded grid only the rows that belong to those years.\nGrouping by grid cell, it averages the predicted values across the three slices, yielding one mean estimate per cell (mean).\nCoastlines are added with rnaturalearth so that land–sea boundaries are clear.\nThe map itself is drawn with ggplot2: each grid cell is a tile coloured by its mean predicted density on a Viridis scale.\nCustomised font sizes ensure that labels remain legible in the exported figure.\nFinally, the graphic is printed to the R graphics device for immediate inspection and saved twice—once as a high-resolution JPEG for reports and once as a serialised ggplot object (.rds) in case the user wants to re-open or modify the map later without re-running the entire script.\n\n```{r message=FALSE, warning=FALSE}\n  # map of average model estimations for the latest three years\n  last_ys <- sort(ys)[(length(ys)-2):length(ys)]\n  map <- grid[grid$year %in% last_ys, ]\n  map <- map %>% group_by(c_square, x,y) %>% summarise(mean = mean(pred, na.rm=TRUE))\n\n  xmin <- min(map$x)\n  xmax <- max(map$x)\n  ymin <- min(map$y)\n  ymax <- max(map$y)\n  xl <- c(xmin - (xmax - xmin) * 0.05, xmax + (xmax - xmin) * 0.05)\n  yl <- c(ymin - (ymax - ymin) * 0.05, ymax + (ymax - ymin) * 0.05)\n  x_breaks <- c(round(xmin, 0), round(xmin, 0) + round((xmax - xmin) / 2, 0), round(xmin, 0) + 2 * round((xmax - xmin) / 2, 0))\n  y_breaks <- c(round(ymin, 0), round(ymin, 0) + round((ymax - ymin) / 2, 0), round(ymin, 0) + 2 * round((ymax - ymin) / 2, 0))\n  \n  library(sf)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\n  world <- ne_countries(scale = \"large\", returnclass = \"sf\")\n  # world <- map_data(\"world\")\np1 <- ggplot() +\n  geom_tile(data = map, aes_string(x = \"x\", y = \"y\", fill = \"mean\")) +\n  scale_fill_viridis_c(option = \"D\", direction = -1) +\n  scale_x_continuous(breaks = x_breaks) +\n  scale_y_continuous(breaks = y_breaks) +\n  geom_sf(data = world, fill = \"lightgrey\", color = \"darkgrey\", linewidth = 0.3) +\n  coord_sf(xlim = xl, ylim = yl, expand = FALSE) +\n  theme_bw() +\n  xlab(\"Longitude\") +\n  ylab(\"Latitude\") +\n  labs(fill = index_expr) +\n  ggtitle(paste0(datasets_label)) +\n  theme(\n    plot.title = element_text(size = 16, hjust = 0.5),     \n    axis.title = element_text(size = 16),                 \n    axis.text = element_text(size = 13),                  \n    legend.title = element_text(size = 14),                \n    legend.text = element_text(size = 13)                 \n  )\n  print(p1)\n  ggsave(paste(resdir, paste0(datasets,\"_\",response,\"_\",fm[m,\"mod\"],\"_MAP.jpg\"), sep = \"/\"), dpi = 300, width = 9, height = 7)\n  saveRDS(p1,paste(resdir, paste0(datasets,\"_\",response,\"_\",fm[m,\"mod\"],\"_MAP.rds\"), sep = \"/\"))\n```\n\n## Uncertainty estimation\n\nThe analysis at this stage generates haul-level predictions from the fitted GAM, replacing the observed response column with model-based values so that each haul retains its original covariates and sampling design.\nAfterwards, it moves on to quantify uncertainty.\nThe code produces values for model predictions at haul level, then, year by year, draws a number (*nBoot*=1000) random coefficient sets from the GAM’s multivariate-normal posterior.\nEach draw yields a new set of haul densities; averaging these within the year produces a distribution of possible annual means.\nFrom that distribution the script stores the point estimate, a 95 % confidence interval, and a coefficient of variation, placing the results in a summary table.\n\n```{r}\n  # model prediction at the survey stations\n  hauls <- d\n  hauls[,which(colnames(hauls)==response)] <- predict(mod, type = \"response\")\n\n  # simulation from the posterior distribution and calculation of the linear predictor (the right-hand side of GAM equation) for each simulation\n\n  if (fm[m, \"dev.expl\"] > 0) {\n  cat(\"- Estimation of time series' 95% confidence interval: \\n\")\n  cat(paste(dname, \", \", fm[m, 2], \", model\", fm[m, \"mod\"], \"\\n\"))\n\n\n  # create data frame of the time series\n  ts <- data.frame(matrix(ncol = 5, nrow = length(ys)))\n  colnames(ts) <- c(\"year\", \"mean\", \"lower\", \"higher\", \"cv\")\n\n  # predictions by year\n  y=1\n  for (y in 1:length(ys)) {\n    n.row <- nrow(hauls[hauls$year==ys[y],])\n    Xp.1 = predict(mod, newdata = hauls[hauls$year==ys[y],], type = \"lpmatrix\")\n    OS.pos = numeric(nrow(hauls[hauls$year==ys[y],]))\n    terms.pos = terms(mod)\n\n    Xp.1 <- predict(mod, newdata = hauls[hauls$year==ys[y],], type = \"lpmatrix\")\n    brp.1 <- mvrnorm(n = nBoot, coef(mod), mod$Vp)\n    rep1 <- exp(Xp.1 %*% t(brp.1))\n    cv <- round(apply(rep1, 1, function(x) sd(x) / mean(x)), 2)\n    cv <- mean(cv)\n\n    if (!is.null(mod$offset)) {\n      off.num.pos <- attr(terms.pos, \"offset\")\n      for (i in off.num.pos) OS.pos <- OS.pos + eval(attr(terms.pos,\"variables\")[[i + 1]], hauls[hauls$year==ys[y],])\n    }\n    p.1 = Xp.1 %*% coef(mod) + OS.pos\n    gPred = exp(p.1)\n    pred_year = sum(exp(p.1))/n.row\n    brp.1 = mvrnorm(n = nBoot, coef(mod), mod$Vp)\n    OS.pos = matrix(0, nrow(hauls[hauls$year==ys[y],]), nBoot)\n    terms.pos = terms(mod)\n    if (!is.null(mod$offset)) {\n      off.num.pos <- attr(terms.pos, \"offset\")\n      for (i in off.num.pos) OS.pos <- OS.pos + eval(attr(terms.pos,\"variables\")[[i + 1]], hauls[hauls$year==ys[y],])\n    }\n    rep1 = data.frame(exp(Xp.1 %*% t(brp.1) + OS.pos))\n    idxSamp = as.numeric(colSums(rep1))/n.row\n    halpha = (1 - 0.95)/2\n    hi <- quantile(idxSamp, 1 - halpha, na.rm = TRUE)\n    lo <- quantile(idxSamp, halpha, na.rm = TRUE)\n    ts[y, 1] <- ys[y]\n    ts[y, 2] <- pred_year \n    ts[y, 3] <- lo\n    ts[y, 4] <- hi\n    ts[y, 5] <- cv\n  }\n\n  # table containing  time series for all the 3 models\n  if (fm[m,\"mod\"]==1) {\n      ts_tab[,c(1:4)] <- ts[,c(1:4)]\n      ts_tab[,11] <- ts[,5]\n  } else if (fm[m,\"mod\"]==2) {\n      ts_tab[,c(5:7)] <- ts[,c(2:4)]\n      ts_tab[,12] <- ts[,5]\n  } else if (fm[m,\"mod\"]==3) {\n      ts_tab[,c(8:10)] <- ts[,c(2:4)]\n      ts_tab[,13] <- ts[,5]\n  }\n\n  head(ts_tab)\n  }\n```\n\n## Timeseries plot\n\nThe code constructs a time-series figure that displays the model’s annual abundance index together with its uncertainty.\nit takes the bootstrap table ts, plots each yearly mean as a blue point joined by a blue line, and draws a semi-transparent ribbon spanning the 95 % confidence limits.\nAll survey years are shown on the x-axis.\nAfter printing the plot to the screen, the script saves it twice: a high-resolution JPEG for reports and an .rds file that preserves the ggplot object for later editing without having to rerun the analysis.\n\n```{r}\n    ## plot of time series\n    if (fm[m, \"dev.expl\"] > 0) {\n  max_val <- max(ts$higher,na.rm=TRUE)\n  p2 <- ggplot()+\n    geom_point(ts,mapping=aes(year,mean),color=\"blue\")+\n    geom_line(ts,mapping=aes(year,mean),color=\"blue\")+\n    geom_ribbon(ts,mapping=aes(ymin=lower,ymax=higher,x=year), alpha = 0.1)+\n    scale_x_continuous(breaks = ys)+\n    theme_bw() +\n    theme(\n          plot.title = element_text(size = 16, hjust = 0.5),     # titolo grafico\n          axis.title = element_text(size = 16),                  # titoli assi\n          axis.text = element_text(size = 13),                   # etichette assi\n          legend.title = element_text(size = 14),                # titolo legenda\n          legend.text = element_text(size = 13)                  # testo legenda\n      )+\n    xlab(\"Year\") +\n    ylab(index_expr)+\n    ggtitle(paste(datasets_label)) + \n    theme(plot.title = element_text(hjust = 0.5))\n  print(p2)\n  ggsave(paste(resdir, paste0(dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \"_TimeSeries_CI.jpg\"), sep = \"/\"), dpi = 300, width = 9, height = 7)\n  saveRDS(p2,paste(resdir, paste0(dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \"_TimeSeries_CI.rds\"), sep = \"/\"))\n  } # if dev.exp > 0\n```\n\n# Analysis on Model 2\n\nThe workflow shown for Model 1 is executed again—line for line for Model 2 (spatial smooth + categorical year) and Model 3 (spatial smooth + linear trend).\nEach model is fitted with the correct family, its object and summary are saved, diagnostic splines are plotted, grid-level predictions and three-year maps are saved out, bootstrap time-series indices are generated, and the same JPEG/RDS files are exported.\nAt each pass the master tables fm and ts_tab are updated so that, once all three loops finish, the user have a complete and directly comparable set of outputs for every candidate model.\n\n```{r echo=FALSE, message=FALSE, warning=FALSE}\n\nm = 2 # to select model 2\n\nif (response == \"pa\") {\n  mod <- suppressWarnings(gam(\n    formula(fm[m, \"formula\"]),\n    family = binomial(\"logit\"),\n    method = \"REML\",\n    data = d,\n    gamma = 1.4\n  ))\n} else if (response == \"n_km2\") {\n  mod <- suppressWarnings(gam(\n    formula(fm[m, \"formula\"]),\n    tw(),\n    method = \"REML\",\n    data = d,\n    gamma = 1.4\n  ))\n} else if (response == \"kg_km2\") {\n  mod <- suppressWarnings(gam(\n    formula(fm[m, \"formula\"]),\n    method = \"REML\",\n    tw(),\n    data = d,\n    gamma = 1.4\n  ))\n} else if (response == \"n_h\") {\n  mod <- suppressWarnings(gam(\n    formula(fm[m, \"formula\"]),\n    tw(),\n    method = \"REML\",\n    data = d,\n    gamma = 1.4\n  ))\n} else if (response == \"kg_km2\") {\n  mod <- suppressWarnings(gam(\n    formula(fm[m, \"formula\"]),\n    method = \"REML\",\n    tw(),\n    data = d,\n    gamma = 1.4\n  ))\n}\n\n# save gam model object\nsaveRDS(mod, file.path(resdir, paste0(dname, \"_\", fm[m, \"response\"], \"_\", fm[m, \"mod\"], \"_MODEL.rds\")))\n\n# save model summary\nsum <- summary(mod)\nsum\nsink(file.path(resdir, paste0(dname, \"_\", fm[m, \"response\"], \"_\", fm[m, \"mod\"], \"_summary.txt\")))\nprint(sum)\nprint(paste(\"AIC: \", AIC(mod)))\nsink()\n\n# store model statistics in fm table\nfm[m, \"dev.expl\"] <- round(sum$dev.expl * 100, 2)\nfm[m, \"AIC\"] <- AIC(mod)\nAIC(mod)\n\n# visualisation plot of splines\npar(mfrow = c(1, 2))\nplot(mod,\n     all.terms = TRUE,\n     scheme = 2,\n     shade = TRUE)\n\n# save plot of splines\nb <- getViz(mod)\njpeg(\n  paste(resdir, paste0(dname, \"_\", fm[m, 2], \"_\", fm[m, \"mod\"], \"_splines.jpg\"), sep = \"/\"),\n  width = 3000,\n  height = 1500,\n  res = 300\n)\npar(mfrow = c(1, 2))\nplot(mod,\n     all.terms = TRUE,\n     scheme = 2,\n     shade = TRUE)\ndev.off()\n\n# predictions + maps plot\ngrid$pred <- NA\ngrid$pred <- predict(mod, newdata = grid, type = \"response\")\n# write.table(grid, file.path(data_dir, paste0(\"Litter_\", dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \".csv\")), sep = \";\", row.names = FALSE)\nwrite.table(grid,\n            file.path(resdir, paste0(\"Litter_\", dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \".csv\")),\n            sep = \";\",\n            row.names = FALSE)\n\n# map of average model estimations for the latest three years\nlast_ys <- sort(ys)[(length(ys) - 2):length(ys)]\nmap <- grid[grid$year %in% last_ys, ]\nmap <- map %>% group_by(c_square, x, y) %>% summarise(mean = mean(pred, na.rm =\n                                                                    TRUE))\n\nxmin <- min(map$x)\nxmax <- max(map$x)\nymin <- min(map$y)\nymax <- max(map$y)\nxl <- c(xmin - (xmax - xmin) * 0.05, xmax + (xmax - xmin) * 0.05)\nyl <- c(ymin - (ymax - ymin) * 0.05, ymax + (ymax - ymin) * 0.05)\nx_breaks <- c(round(xmin, 0),\n              round(xmin, 0) + round((xmax - xmin) / 2, 0),\n              round(xmin, 0) + 2 * round((xmax - xmin) / 2, 0))\ny_breaks <- c(round(ymin, 0),\n              round(ymin, 0) + round((ymax - ymin) / 2, 0),\n              round(ymin, 0) + 2 * round((ymax - ymin) / 2, 0))\n\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nworld <- ne_countries(scale = \"large\", returnclass = \"sf\")\n# world <- map_data(\"world\")\np1 <- ggplot() +\n  geom_tile(data = map, aes_string(x = \"x\", y = \"y\", fill = \"mean\")) +\n  scale_fill_viridis_c(option = \"D\", direction = -1) +\n  scale_x_continuous(breaks = x_breaks) +\n  scale_y_continuous(breaks = y_breaks) +\n  geom_sf(\n    data = world,\n    fill = \"lightgrey\",\n    color = \"darkgrey\",\n    linewidth = 0.3\n  ) +\n  coord_sf(xlim = xl,\n           ylim = yl,\n           expand = FALSE) +\n  theme_bw() +\n  xlab(\"Longitude\") +\n  ylab(\"Latitude\") +\n  labs(fill = index_expr) +\n  ggtitle(paste0(datasets_label)) +\n  theme(\n    plot.title = element_text(size = 16, hjust = 0.5),\n    axis.title = element_text(size = 16),\n    axis.text = element_text(size = 13),\n    legend.title = element_text(size = 14),\n    legend.text = element_text(size = 13)\n  )\nprint(p1)\nggsave(\n  paste(\n    resdir,\n    paste0(datasets, \"_\", response, \"_\", fm[m, \"mod\"], \"_MAP.jpg\"),\n    sep = \"/\"\n  ),\n  dpi = 300,\n  width = 9,\n  height = 7\n)\nsaveRDS(p1, paste(\n  resdir,\n  paste0(datasets, \"_\", response, \"_\", fm[m, \"mod\"], \"_MAP.rds\"),\n  sep = \"/\"\n))\n\n# model prediction at the survey stations\nhauls <- d\nhauls[, which(colnames(hauls) == response)] <- predict(mod, type = \"response\")\n\n# simulation from the posterior distribution and calculation of the linear predictor (the right-hand side of GAM equation) for each simulation\n\nif (fm[m, \"dev.expl\"] > 0) {\n  # create data frame of the time series\n  ts <- data.frame(matrix(ncol = 5, nrow = length(ys)))\n  colnames(ts) <- c(\"year\", \"mean\", \"lower\", \"higher\", \"cv\")\n  \n  # predictions by year\n  y = 1\n  for (y in 1:length(ys)) {\n    n.row <- nrow(hauls[hauls$year == ys[y], ])\n    Xp.1 = predict(mod, newdata = hauls[hauls$year == ys[y], ], type = \"lpmatrix\")\n    OS.pos = numeric(nrow(hauls[hauls$year == ys[y], ]))\n    terms.pos = terms(mod)\n    \n    Xp.1 <- predict(mod, newdata = hauls[hauls$year == ys[y], ], type = \"lpmatrix\")\n    brp.1 <- mvrnorm(n = nBoot, coef(mod), mod$Vp)\n    rep1 <- exp(Xp.1 %*% t(brp.1))\n    cv <- round(apply(rep1, 1, function(x)\n      sd(x) / mean(x)), 2)\n    cv <- mean(cv)\n\n    if (!is.null(mod$offset)) {\n      off.num.pos <- attr(terms.pos, \"offset\")\n      for (i in off.num.pos)\n        OS.pos <- OS.pos + eval(attr(terms.pos, \"variables\")[[i + 1]], \n                                hauls[hauls$year ==ys[y], ])\n    }\n    p.1 = Xp.1 %*% coef(mod) + OS.pos\n    gPred = exp(p.1)\n    pred_year = sum(exp(p.1)) / n.row\n    brp.1 = mvrnorm(n = nBoot, coef(mod), mod$Vp)\n    OS.pos = matrix(0, nrow(hauls[hauls$year == ys[y], ]), nBoot)\n    terms.pos = terms(mod)\n    if (!is.null(mod$offset)) {\n      off.num.pos <- attr(terms.pos, \"offset\")\n      for (i in off.num.pos)\n        OS.pos <- OS.pos + eval(attr(terms.pos, \"variables\")[[i + 1]], \n                                hauls[hauls$year == ys[y], ])\n    }\n    rep1 = data.frame(exp(Xp.1 %*% t(brp.1) + OS.pos))\n    idxSamp = as.numeric(colSums(rep1)) / n.row\n    halpha = (1 - 0.95) / 2\n    hi <- quantile(idxSamp, 1 - halpha, na.rm = TRUE)\n    lo <- quantile(idxSamp, halpha, na.rm = TRUE)\n    ts[y, 1] <- ys[y]\n    ts[y, 2] <- pred_year\n    ts[y, 3] <- lo\n    ts[y, 4] <- hi\n    ts[y, 5] <- cv\n  }\n  \n  # table containing  time series for all the 3 models\n  if (fm[m, \"mod\"] == 1) {\n    ts_tab[, c(1:4)] <- ts[, c(1:4)]\n    ts_tab[, 11] <- ts[, 5]\n  } else if (fm[m, \"mod\"] == 2) {\n    ts_tab[, c(5:7)] <- ts[, c(2:4)]\n    ts_tab[, 12] <- ts[, 5]\n  } else if (fm[m, \"mod\"] == 3) {\n    ts_tab[, c(8:10)] <- ts[, c(2:4)]\n    ts_tab[, 13] <- ts[, 5]\n  }\n  \n  \n  ## plot of time series\n  max_val <- max(ts$higher, na.rm = TRUE)\n  p2 <- ggplot() +\n    geom_point(ts, mapping = aes(year, mean), color = \"blue\") +\n    geom_line(ts, mapping = aes(year, mean), color = \"blue\") +\n    geom_ribbon(ts,\n                mapping = aes(ymin = lower, ymax = higher, x = year),\n                alpha = 0.1) +\n    scale_x_continuous(breaks = ys) +\n    theme_bw() +\n    theme(\n      plot.title = element_text(size = 16, hjust = 0.5),\n      axis.title = element_text(size = 16),\n      axis.text = element_text(size = 13),\n      legend.title = element_text(size = 14),\n      legend.text = element_text(size = 13) \n    ) +\n    xlab(\"Year\") +\n    ylab(index_expr) +\n    ggtitle(paste(datasets_label)) +\n    theme(plot.title = element_text(hjust = 0.5))\n  print(p2)\n  ggsave(\n    paste(\n      resdir,\n      paste0(dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \"_TimeSeries_CI.jpg\"),\n      sep = \"/\"\n    ),\n    dpi = 300,\n    width = 7,\n    height = 7\n  )\n  saveRDS(p2, paste(\n    resdir,\n    paste0(dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \"_TimeSeries_CI.rds\"),\n    sep = \"/\"\n  ))\n} # if dev.exp > 0\n\n```\n\n# Analysis on Model 3\n\nThe workflow shown for Model 1 and 2 is executed again—line for line for Model 3 (spatial smooth + linear trend). At each step the master tables *fm* and *ts_tab* are updated so that, once all three loops finish, the user have a complete and directly comparable set of outputs for every candidate model.\n\n```{r echo=FALSE, message=FALSE, warning=FALSE}\nm = 3 # to select model 3\n\nif (response == \"pa\") {\n  mod <- suppressWarnings(gam(\n    formula(fm[m, \"formula\"]),\n    family = binomial(\"logit\"),\n    method = \"REML\",\n    data = d,\n    gamma = 1.4\n  ))\n} else if (response == \"n_km2\") {\n  mod <- suppressWarnings(gam(\n    formula(fm[m, \"formula\"]),\n    tw(),\n    method = \"REML\",\n    data = d,\n    gamma = 1.4\n  ))\n} else if (response == \"kg_km2\") {\n  mod <- suppressWarnings(gam(\n    formula(fm[m, \"formula\"]),\n    method = \"REML\",\n    tw(),\n    data = d,\n    gamma = 1.4\n  ))\n} else if (response == \"n_h\") {\n  mod <- suppressWarnings(gam(\n    formula(fm[m, \"formula\"]),\n    tw(),\n    method = \"REML\",\n    data = d,\n    gamma = 1.4\n  ))\n} else if (response == \"kg_km2\") {\n  mod <- suppressWarnings(gam(\n    formula(fm[m, \"formula\"]),\n    method = \"REML\",\n    tw(),\n    data = d,\n    gamma = 1.4\n  ))\n}\n\n# save gam model object\nsaveRDS(mod, file.path(resdir, paste0(dname, \"_\", fm[m, \"response\"], \"_\", fm[m, \"mod\"], \"_MODEL.rds\")))\n\n# save model summary\nsum <- summary(mod)\nsum\nsink(file.path(resdir, paste0(dname, \"_\", fm[m, \"response\"], \"_\", fm[m, \"mod\"], \"_summary.txt\")))\nprint(sum)\nprint(paste(\"AIC: \", AIC(mod)))\nsink()\n\n# store model statistics in fm table\nfm[m, \"dev.expl\"] <- round(sum$dev.expl * 100, 2)\nfm[m, \"AIC\"] <- AIC(mod)\nAIC(mod)\n\n# visualisation plot of splines\npar(mfrow = c(1, 2))\nplot(mod,\n     all.terms = TRUE,\n     scheme = 2,\n     shade = TRUE)\n\n# save plot of splines\nb <- getViz(mod)\njpeg(\n  paste(resdir, paste0(dname, \"_\", fm[m, 2], \"_\", fm[m, \"mod\"], \"_splines.jpg\"), sep = \"/\"),\n  width = 3000,\n  height = 1500,\n  res = 300\n)\npar(mfrow = c(1, 2))\nplot(mod,\n     all.terms = TRUE,\n     scheme = 2,\n     shade = TRUE)\ndev.off()\n\n# predictions + maps plot\ngrid$pred <- NA\ngrid$pred <- predict(mod, newdata = grid, type = \"response\")\n# write.table(grid, file.path(data_dir, paste0(\"Litter_\", dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \".csv\")), sep = \";\", row.names = FALSE)\nwrite.table(grid,\n            file.path(resdir, paste0(\"Litter_\", dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \".csv\")),\n            sep = \";\",\n            row.names = FALSE)\n\n# map of average model estimations for the latest three years\nlast_ys <- sort(ys)[(length(ys) - 2):length(ys)]\nmap <- grid[grid$year %in% last_ys, ]\nmap <- map %>% group_by(c_square, x, y) %>% summarise(mean = mean(pred, na.rm =\n                                                                    TRUE))\n\nxmin <- min(map$x)\nxmax <- max(map$x)\nymin <- min(map$y)\nymax <- max(map$y)\nxl <- c(xmin - (xmax - xmin) * 0.05, xmax + (xmax - xmin) * 0.05)\nyl <- c(ymin - (ymax - ymin) * 0.05, ymax + (ymax - ymin) * 0.05)\nx_breaks <- c(round(xmin, 0),\n              round(xmin, 0) + round((xmax - xmin) / 2, 0),\n              round(xmin, 0) + 2 * round((xmax - xmin) / 2, 0))\ny_breaks <- c(round(ymin, 0),\n              round(ymin, 0) + round((ymax - ymin) / 2, 0),\n              round(ymin, 0) + 2 * round((ymax - ymin) / 2, 0))\n\nlibrary(sf)\nlibrary(rnaturalearth)\nlibrary(rnaturalearthdata)\nworld <- ne_countries(scale = \"large\", returnclass = \"sf\")\n# world <- map_data(\"world\")\np1 <- ggplot() +\n  geom_tile(data = map, aes_string(x = \"x\", y = \"y\", fill = \"mean\")) +\n  scale_fill_viridis_c(option = \"D\", direction = -1) +\n  scale_x_continuous(breaks = x_breaks) +\n  scale_y_continuous(breaks = y_breaks) +\n  geom_sf(\n    data = world,\n    fill = \"lightgrey\",\n    color = \"darkgrey\",\n    linewidth = 0.3\n  ) +\n  coord_sf(xlim = xl,\n           ylim = yl,\n           expand = FALSE) +\n  theme_bw() +\n  xlab(\"Longitude\") +\n  ylab(\"Latitude\") +\n  labs(fill = index_expr) +\n  ggtitle(paste0(datasets_label)) +\n  theme(\n    plot.title = element_text(size = 16, hjust = 0.5),\n    axis.title = element_text(size = 16),\n    axis.text = element_text(size = 13),\n    legend.title = element_text(size = 14),\n    legend.text = element_text(size = 13)\n  )\nprint(p1)\nggsave(\n  paste(\n    resdir,\n    paste0(datasets, \"_\", response, \"_\", fm[m, \"mod\"], \"_MAP.jpg\"),\n    sep = \"/\"\n  ),\n  dpi = 300,\n  width = 9,\n  height = 7\n)\nsaveRDS(p1, paste(\n  resdir,\n  paste0(datasets, \"_\", response, \"_\", fm[m, \"mod\"], \"_MAP.rds\"),\n  sep = \"/\"\n))\n\n# model prediction at the survey stations\nhauls <- d\nhauls[, which(colnames(hauls) == response)] <- predict(mod, type = \"response\")\n\n# simulation from the posterior distribution and calculation of the linear predictor (the right-hand side of GAM equation) for each simulation\n\nif (fm[m, \"dev.expl\"] > 0) {\n  # create data frame of the time series\n  ts <- data.frame(matrix(ncol = 5, nrow = length(ys)))\n  colnames(ts) <- c(\"year\", \"mean\", \"lower\", \"higher\", \"cv\")\n  \n  # predictions by year\n  y = 1\n  for (y in 1:length(ys)) {\n    n.row <- nrow(hauls[hauls$year == ys[y], ])\n    Xp.1 = predict(mod, newdata = hauls[hauls$year == ys[y], ], type = \"lpmatrix\")\n    OS.pos = numeric(nrow(hauls[hauls$year == ys[y], ]))\n    terms.pos = terms(mod)\n    \n    Xp.1 <- predict(mod, newdata = hauls[hauls$year == ys[y], ], type = \"lpmatrix\")\n    brp.1 <- mvrnorm(n = nBoot, coef(mod), mod$Vp)\n    rep1 <- exp(Xp.1 %*% t(brp.1))\n    cv <- round(apply(rep1, 1, function(x)\n      sd(x) / mean(x)), 2)\n    cv <- mean(cv)\n\n    if (!is.null(mod$offset)) {\n      off.num.pos <- attr(terms.pos, \"offset\")\n      for (i in off.num.pos)\n        OS.pos <- OS.pos + eval(attr(terms.pos, \"variables\")[[i + 1]], \n                                hauls[hauls$year ==ys[y], ])\n    }\n    p.1 = Xp.1 %*% coef(mod) + OS.pos\n    gPred = exp(p.1)\n    pred_year = sum(exp(p.1)) / n.row\n    brp.1 = mvrnorm(n = nBoot, coef(mod), mod$Vp)\n    OS.pos = matrix(0, nrow(hauls[hauls$year == ys[y], ]), nBoot)\n    terms.pos = terms(mod)\n    if (!is.null(mod$offset)) {\n      off.num.pos <- attr(terms.pos, \"offset\")\n      for (i in off.num.pos)\n        OS.pos <- OS.pos + eval(attr(terms.pos, \"variables\")[[i + 1]], \n                                hauls[hauls$year == ys[y], ])\n    }\n    rep1 = data.frame(exp(Xp.1 %*% t(brp.1) + OS.pos))\n    idxSamp = as.numeric(colSums(rep1)) / n.row\n    halpha = (1 - 0.95) / 2\n    hi <- quantile(idxSamp, 1 - halpha, na.rm = TRUE)\n    lo <- quantile(idxSamp, halpha, na.rm = TRUE)\n    ts[y, 1] <- ys[y]\n    ts[y, 2] <- pred_year\n    ts[y, 3] <- lo\n    ts[y, 4] <- hi\n    ts[y, 5] <- cv\n  }\n  \n  # table containing  time series for all the 3 models\n  if (fm[m, \"mod\"] == 1) {\n    ts_tab[, c(1:4)] <- ts[, c(1:4)]\n    ts_tab[, 11] <- ts[, 5]\n  } else if (fm[m, \"mod\"] == 2) {\n    ts_tab[, c(5:7)] <- ts[, c(2:4)]\n    ts_tab[, 12] <- ts[, 5]\n  } else if (fm[m, \"mod\"] == 3) {\n    ts_tab[, c(8:10)] <- ts[, c(2:4)]\n    ts_tab[, 13] <- ts[, 5]\n  }\n  \n  \n  ## plot of time series\n  max_val <- max(ts$higher, na.rm = TRUE)\n  p2 <- ggplot() +\n    geom_point(ts, mapping = aes(year, mean), color = \"blue\") +\n    geom_line(ts, mapping = aes(year, mean), color = \"blue\") +\n    geom_ribbon(ts,\n                mapping = aes(ymin = lower, ymax = higher, x = year),\n                alpha = 0.1) +\n    scale_x_continuous(breaks = ys) +\n    theme_bw() +\n    theme(\n      plot.title = element_text(size = 16, hjust = 0.5),\n      axis.title = element_text(size = 16),\n      axis.text = element_text(size = 13),\n      legend.title = element_text(size = 14),\n      legend.text = element_text(size = 13) \n    ) +\n    xlab(\"Year\") +\n    ylab(index_expr) +\n    ggtitle(paste(datasets_label)) +\n    theme(plot.title = element_text(hjust = 0.5))\n  print(p2)\n  ggsave(\n    paste(\n      resdir,\n      paste0(dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \"_TimeSeries_CI.jpg\"),\n      sep = \"/\"\n    ),\n    dpi = 300,\n    width = 7,\n    height = 7\n  )\n  saveRDS(p2, paste(\n    resdir,\n    paste0(dname, \"_\", fm[m, 2], \"_\", fm[m, 3], \"_TimeSeries_CI.rds\"),\n    sep = \"/\"\n  ))\n} # if dev.exp > 0\n```\n\n\n# Merging plots and results\n\nAt the very end of the script everything comes together.\nThe bootstrap table *ts_tab*, which now holds the annual indices and confidence limits for all three candidate models, is reshaped into a long format so that the three trajectories are reported on the same column.\n*ggplot* is then called to build a comparative time-series figure.\nEach model’s curve is shown as points and lines, coloured black for Model 1, green for Model 2 and blue for Model 3.\nIf Model 1 produced confidence limits, its ribbon is added in semi-transparent grey beneath the three lines to give a visual gauge of uncertainty.\nAfter rendering to the screen the plot is exported in two forms: a high-resolution JPEG for static reporting and an RDS object that preserves the ggplot object for future editing.\nFinally, two comma-separated files are written to the results directory.\nThe first (e.g. *Results_summary_n_km2_SUP.csv*) stores the fm table, which summarises deviance explained and AIC for each model; the second ( *Time_series_n_km2_SUP.csv*) contains the complete bootstrap table *ts_tab*.\nThese text files give a concise record of model performance and yearly indices that can be inspected or imported into other software without opening R.\n\n```{r}\n# plot time series of the three models together\nts_tab$year <- ys\nts_long <- reshape2::melt(ts_tab[,c(1,2,5,8)],id.vars=\"year\")\nts_long$model <- substr(as.character(ts_long$variable),nchar(as.character(ts_long$variable)),nchar(as.character(ts_long$variable)))\nts_long <- ts_long[!is.na(ts_long$value),]\n\nif (fm[1, \"dev.expl\"] > 0) {\n  p3 <- ggplot()+\n    geom_point(ts_long,mapping=aes(year,value,color=model))+\n    geom_line(ts_long,mapping=aes(year,value,color=model))+\n    scale_color_manual(values = c(\"1\" = \"black\", \"2\" = \"green\",\"3\"=\"blue\"))+\n    geom_ribbon(ts_tab,mapping=aes(ymin=lower_1,ymax=higher_1,x=year), alpha = 0.1)+\n    scale_x_continuous(breaks = ys)+\n    theme_bw() +\n    theme(\n      plot.title = element_text(size = 16, hjust = 0.5),\n      axis.title = element_text(size = 16),\n      axis.text = element_text(size = 13),\n      legend.title = element_text(size = 14),\n      legend.text = element_text(size = 13) \n    ) +\n    xlab(\"Year\") +\n    ylab(index_expr)+\n    ggtitle(paste(datasets_label))+ \n    theme(plot.title = element_text(hjust = 0.5))\n} else {\n  p3 <- ggplot()+\n    geom_point(ts_long,mapping=aes(year,value,color=model))+\n    geom_line(ts_long,mapping=aes(year,value,color=model))+\n    scale_color_manual(values = c(\"1\" = \"black\", \"2\" = \"green\",\"3\"=\"blue\"))+\n    scale_x_continuous(breaks = ys)+\n    theme_bw() +\n    theme(\n      plot.title = element_text(size = 16, hjust = 0.5),\n      axis.title = element_text(size = 16),\n      axis.text = element_text(size = 13),\n      legend.title = element_text(size = 14),\n      legend.text = element_text(size = 13) \n    ) +\n    xlab(\"Year\") +\n    ylab(index_expr)+\n    ggtitle(paste(datasets_label))+ \n    theme(plot.title = element_text(hjust = 0.5))\n}\n\nprint(p3)\nggsave(paste(resdir, paste0(dname, \"_\", response, \"_TimeSeries_CI_models.jpg\"), sep = \"/\"), dpi = 300, width = 9, height = 7)\nsaveRDS(p3,paste(resdir, paste0(dname, \"_\", response, \"_TimeSeries_CI_models.rds\"), sep = \"/\"))\n\nwrite.table(fm, file.path(resdir, paste0(\"Results_summary_\",response,\"_\",datasets,\".csv\")), sep = \";\", row.names = FALSE)\n\nts_tab$area = AREA\n\nwrite.table(ts_tab, file.path(resdir, paste0(\"_Time_series_\",response,\"_\",datasets,\".csv\")), sep = \";\", row.names = FALSE)\n```\n\n"},"formats":{"html":{"identifier":{"display-name":"HTML","target-format":"html","base-format":"html"},"execute":{"fig-width":7,"fig-height":5,"fig-format":"retina","fig-dpi":96,"df-print":"paged","error":false,"eval":true,"cache":null,"freeze":false,"echo":true,"output":true,"warning":false,"include":true,"keep-md":false,"keep-ipynb":false,"ipynb":null,"enabled":null,"daemon":null,"daemon-restart":false,"debug":false,"ipynb-filters":[],"ipynb-shell-interactivity":null,"plotly-connected":true,"message":false,"engine":"knitr"},"render":{"keep-tex":false,"keep-typ":false,"keep-source":false,"keep-hidden":false,"prefer-html":false,"output-divs":true,"output-ext":"html","fig-align":"default","fig-pos":null,"fig-env":null,"code-fold":"none","code-overflow":"scroll","code-link":false,"code-line-numbers":false,"code-tools":false,"tbl-colwidths":"auto","merge-includes":true,"inline-includes":false,"preserve-yaml":false,"latex-auto-mk":true,"latex-auto-install":true,"latex-clean":true,"latex-min-runs":1,"latex-max-runs":10,"latex-makeindex":"makeindex","latex-makeindex-opts":[],"latex-tlmgr-opts":[],"latex-input-paths":[],"latex-output-dir":null,"link-external-icon":false,"link-external-newwindow":false,"self-contained-math":false,"format-resources":[],"notebook-links":true},"pandoc":{"standalone":true,"wrap":"none","default-image-extension":"png","to":"html","toc":true,"toc-depth":3,"number-sections":false,"include-in-header":["assets/icons.html"],"output-file":"litter_modelling.html"},"language":{"toc-title-document":"Table of contents","toc-title-website":"On this page","related-formats-title":"Other Formats","related-notebooks-title":"Notebooks","source-notebooks-prefix":"Source","other-links-title":"Other Links","code-links-title":"Code Links","launch-dev-container-title":"Launch Dev Container","launch-binder-title":"Launch Binder","article-notebook-label":"Article Notebook","notebook-preview-download":"Download Notebook","notebook-preview-download-src":"Download Source","notebook-preview-back":"Back to Article","manuscript-meca-bundle":"MECA Bundle","section-title-abstract":"Abstract","section-title-appendices":"Appendices","section-title-footnotes":"Footnotes","section-title-references":"References","section-title-reuse":"Reuse","section-title-copyright":"Copyright","section-title-citation":"Citation","appendix-attribution-cite-as":"For attribution, please cite this work as:","appendix-attribution-bibtex":"BibTeX citation:","appendix-view-license":"View License","title-block-author-single":"Author","title-block-author-plural":"Authors","title-block-affiliation-single":"Affiliation","title-block-affiliation-plural":"Affiliations","title-block-published":"Published","title-block-modified":"Modified","title-block-keywords":"Keywords","callout-tip-title":"Tip","callout-note-title":"Note","callout-warning-title":"Warning","callout-important-title":"Important","callout-caution-title":"Caution","code-summary":"Code","code-tools-menu-caption":"Code","code-tools-show-all-code":"Show All Code","code-tools-hide-all-code":"Hide All Code","code-tools-view-source":"View Source","code-tools-source-code":"Source Code","tools-share":"Share","tools-download":"Download","code-line":"Line","code-lines":"Lines","copy-button-tooltip":"Copy to Clipboard","copy-button-tooltip-success":"Copied!","repo-action-links-edit":"Edit this page","repo-action-links-source":"View source","repo-action-links-issue":"Report an issue","back-to-top":"Back to top","search-no-results-text":"No results","search-matching-documents-text":"matching documents","search-copy-link-title":"Copy link to search","search-hide-matches-text":"Hide additional matches","search-more-match-text":"more match in this document","search-more-matches-text":"more matches in this document","search-clear-button-title":"Clear","search-text-placeholder":"","search-detached-cancel-button-title":"Cancel","search-submit-button-title":"Submit","search-label":"Search","toggle-section":"Toggle section","toggle-sidebar":"Toggle sidebar navigation","toggle-dark-mode":"Toggle dark mode","toggle-reader-mode":"Toggle reader mode","toggle-navigation":"Toggle navigation","crossref-fig-title":"Figure","crossref-tbl-title":"Table","crossref-lst-title":"Listing","crossref-thm-title":"Theorem","crossref-lem-title":"Lemma","crossref-cor-title":"Corollary","crossref-prp-title":"Proposition","crossref-cnj-title":"Conjecture","crossref-def-title":"Definition","crossref-exm-title":"Example","crossref-exr-title":"Exercise","crossref-ch-prefix":"Chapter","crossref-apx-prefix":"Appendix","crossref-sec-prefix":"Section","crossref-eq-prefix":"Equation","crossref-lof-title":"List of Figures","crossref-lot-title":"List of Tables","crossref-lol-title":"List of Listings","environment-proof-title":"Proof","environment-remark-title":"Remark","environment-solution-title":"Solution","listing-page-order-by":"Order By","listing-page-order-by-default":"Default","listing-page-order-by-date-asc":"Oldest","listing-page-order-by-date-desc":"Newest","listing-page-order-by-number-desc":"High to Low","listing-page-order-by-number-asc":"Low to High","listing-page-field-date":"Date","listing-page-field-title":"Title","listing-page-field-description":"Description","listing-page-field-author":"Author","listing-page-field-filename":"File Name","listing-page-field-filemodified":"Modified","listing-page-field-subtitle":"Subtitle","listing-page-field-readingtime":"Reading Time","listing-page-field-wordcount":"Word Count","listing-page-field-categories":"Categories","listing-page-minutes-compact":"{0} min","listing-page-category-all":"All","listing-page-no-matches":"No matching items","listing-page-words":"{0} words","listing-page-filter":"Filter","draft":"Draft"},"metadata":{"lang":"en","fig-responsive":true,"quarto-version":"1.7.33","toc-location":"left","theme":"default","smooth-scroll":true,"title":"Modelling analysis on Litter data","author":"M. T. Spedicato, W. Zupa"},"extensions":{"book":{"multiFile":true}}}},"projectFormats":["html"]}